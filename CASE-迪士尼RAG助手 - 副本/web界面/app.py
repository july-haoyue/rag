from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import sys
import os
import re  # æ·»åŠ æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—å¯¼å…¥
import math  # æ·»åŠ æ•°å­¦æ¨¡å—å¯¼å…¥ï¼Œç”¨äºè®¡ç®—BM25ç›¸å…³åˆ†æ•°

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å…ˆåº”ç”¨huggingface_hubè¡¥ä¸ï¼Œè§£å†³å¯èƒ½çš„APIå˜æ›´é—®é¢˜
try:
    from patch_huggingface_hub import apply_patch
    apply_patch()
    print("âœ… å·²åº”ç”¨huggingface_hubè¡¥ä¸")
except Exception as e:
    print(f"âš ï¸ åº”ç”¨huggingface_hubè¡¥ä¸æ—¶å‡ºé”™: {str(e)}")
    # å³ä½¿è¡¥ä¸åº”ç”¨å¤±è´¥ä¹Ÿç»§ç»­è¿è¡Œ

# å¯¼å…¥è¿ªå£«å°¼RAGåŠ©æ‰‹ï¼ˆä½¿ç”¨FAISSå‘é‡ç‰ˆï¼‰
from è¿ªå£«å°¼RAGæ£€ç´¢åŠ©æ‰‹FAISSç‰ˆ import DisneyRAGAssistant

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
# ä½¿ç”¨ç»å¯¹è·¯å¾„æ¥æŒ‡å®šstatic_folderå’Œtemplate_folder
app = Flask(__name__, template_folder=os.path.join(current_dir, 'templates'), static_folder=os.path.join(current_dir, 'static'))
CORS(app)  # å¯ç”¨CORSæ”¯æŒ

# åˆå§‹åŒ–è¿ªå£«å°¼RAGåŠ©æ‰‹
rag_assistant = None

def init_assistant():
    global rag_assistant  # å…¨å±€å˜é‡å£°æ˜å¿…é¡»åœ¨å‡½æ•°å¼€å¤´
    try:
        # æ˜¾å¼ä½¿ç”¨å…¨å±€çš„osæ¨¡å—
        import os as global_os
        
        # è®¾ç½®æ­£ç¡®çš„ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‡å‘é¡¹ç›®æ ¹ç›®å½•çš„final_indexï¼‰
        project_root = global_os.path.dirname(global_os.path.dirname(global_os.path.abspath(__file__)))
        index_path = global_os.path.join(project_root, 'final_index')
        
        print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"å½“å‰å·¥ä½œç›®å½•: {global_os.getcwd()}")
        
        # ç¡®ä¿ç´¢å¼•ç›®å½•å­˜åœ¨
        if not global_os.path.exists(index_path):
            print(f"é”™è¯¯: ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {index_path}")
            # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„ç´¢å¼•ç›®å½•
            alternative_paths = [
                global_os.path.join(project_root, 'simple_index'),
                global_os.path.join(project_root, 'fixed_index')
            ]
            for alt_path in alternative_paths:
                if global_os.path.exists(alt_path):
                    index_path = alt_path
                    print(f"ä½¿ç”¨å¤‡ç”¨ç´¢å¼•ç›®å½•: {index_path}")
                    break
            else:
                print("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç´¢å¼•ç›®å½•")
                return False
        
        # ä¿®æ”¹å½“å‰å·¥ä½œç›®å½•ä¸ºé¡¹ç›®æ ¹ç›®å½•ï¼Œè¿™æ ·ç´¢å¼•åŠ è½½æ‰èƒ½æ­£ç¡®æ‰¾åˆ°æ–‡ä»¶
        global_os.chdir(project_root)
        print(f"åˆ‡æ¢å·¥ä½œç›®å½•å: {global_os.getcwd()}")
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥
        dashscope_api_key = global_os.getenv("DASHSCOPE_API_KEY", "")
        aliyun_api_key = global_os.getenv("ALIYUN_API_KEY", "")
        
        # ä½¿ç”¨ä»»æ„ä¸€ä¸ªå­˜åœ¨çš„APIå¯†é’¥
        api_key = dashscope_api_key if dashscope_api_key else aliyun_api_key
        
        if not api_key:
            print("è­¦å‘Š: ç¯å¢ƒå˜é‡DASHSCOPE_API_KEYå’ŒALIYUN_API_KEYéƒ½æœªè®¾ç½®")
            print("æç¤º: æ‚¨å¯ä»¥åœ¨Windowså‘½ä»¤æç¤ºç¬¦ä¸­ä½¿ç”¨ set DASHSCOPE_API_KEY=your_api_key æˆ– set ALIYUN_API_KEY=your_api_key æ¥è®¾ç½®ç¯å¢ƒå˜é‡")
        else:
            print("æˆåŠŸè·å–APIå¯†é’¥")
        
        # å°è¯•åˆ›å»ºå®Œæ•´ç‰ˆçš„DisneyRAGAssistant
        try:
            print("å°è¯•åˆ›å»ºå®Œæ•´ç‰ˆDisneyRAGAssistant...")
            rag_assistant = DisneyRAGAssistant(index_path, 
                                             dashscope_api_key=api_key)
            print("âœ… å®Œæ•´ç‰ˆDisneyRAGAssistantåˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as full_error:
            print(f"âš ï¸ åˆ›å»ºå®Œæ•´ç‰ˆåŠ©æ‰‹å¤±è´¥: {str(full_error)}")
            print("ç»§ç»­åˆ›å»ºç®€åŒ–ç‰ˆåŠ©æ‰‹...")
        
        # åˆ›å»ºä¸€ä¸ªåŸºæœ¬ç‰ˆæœ¬çš„åŠ©æ‰‹ï¼Œè·³è¿‡å¤æ‚çš„åµŒå…¥æ¨¡å‹å’Œå‘é‡ç´¢å¼•åˆå§‹åŒ–
        import json
        
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„åŠ©æ‰‹ç±»ï¼Œåªå®ç°åŸºæœ¬çš„å…³é”®è¯æœç´¢åŠŸèƒ½
        class SimpleDisneyAssistant:
            def __init__(self, index_dir):
                self.index_dir = index_dir
                self.inverted_index = {}
                self.chunk_mapping = []
                self.initialized = False
                # ç®€å•çš„åœç”¨è¯è¡¨
                self.stop_words = {
                    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
                    'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 
                    'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
                }
            
            def load_index(self):
                try:
                    # ä½¿ç”¨global_osæ¥é¿å…åç§°å†²çª
                    # åªåŠ è½½å¿…è¦çš„ç´¢å¼•æ–‡ä»¶
                    index_path = global_os.path.join(self.index_dir, 'simple_index.json')
                    with open(index_path, 'r', encoding='utf-8') as f:
                        index_data = json.load(f)
                    self.inverted_index = index_data['inverted_index']
                    
                    # åŠ è½½åˆ‡ç‰‡æ˜ å°„
                    mapping_path = global_os.path.join(self.index_dir, 'chunk_mapping.json')
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        self.chunk_mapping = json.load(f)
                    
                    self.initialized = True
                    print("âœ… ç®€åŒ–ç‰ˆç´¢å¼•åŠ è½½æˆåŠŸ")
                    print(f"- ç´¢å¼•è¯æ•°é‡: {len(self.inverted_index)}")
                    print(f"- çŸ¥è¯†åº“åˆ‡ç‰‡æ•°: {len(self.chunk_mapping)}")
                    return True
                except Exception as e:
                    print(f"âŒ ç®€åŒ–ç‰ˆç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
                    return False
            
            def preprocess_query(self, query):
                # ç®€å•çš„å…³é”®è¯æå–
                import re
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                query = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', query)
                query = query.strip().lower()
                
                # æå–å…³é”®è¯
                keywords = []
                # ç®€å•çš„ä¸­æ–‡è¯ç»„æå–
                chinese_pattern = r'[\u4e00-\u9fa5]'
                chinese_chars = re.findall(chinese_pattern, query)
                
                # æå–2-4å­—çš„è¯ç»„
                for i in range(len(chinese_chars)):
                    if i + 1 < len(chinese_chars):
                        word2 = chinese_chars[i] + chinese_chars[i+1]
                        if word2 not in self.stop_words:
                            keywords.append(word2)
                
                # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢çš„ä¸»è¦éƒ¨åˆ†
                if not keywords and len(query) > 0:
                    keywords = [query[:10]]
                
                return keywords
            
            def search(self, query, top_k=5, min_score=0.1, use_vector_search=False):
                if not self.initialized:
                    print("âš ï¸ è¯·å…ˆåŠ è½½ç´¢å¼•")
                    return []
                
                keywords = self.preprocess_query(query)
                print(f"å¢å¼ºç‰ˆæœç´¢å…³é”®è¯: {keywords}")
                
                if not keywords:
                    return []
                
                # è®¡ç®—æ–‡æ¡£æ€»æ•°
                total_docs = len(self.chunk_mapping)
                
                # åŸºäºBM25å¯å‘çš„æœç´¢ç®—æ³•
                chunk_scores = {}
                
                # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„æ–‡æ¡£é¢‘ç‡ï¼ˆDFï¼‰
                keyword_df = {}
                for keyword in keywords:
                    if keyword in self.inverted_index:
                        keyword_df[keyword] = len(self.inverted_index[keyword])
                    else:
                        keyword_df[keyword] = 0
                
                # 1. å€’æ’ç´¢å¼•ç²¾ç¡®åŒ¹é…
                for keyword in keywords:
                    if keyword in self.inverted_index:
                        # è®¡ç®—IDF (é€†æ–‡æ¡£é¢‘ç‡)
                        if keyword_df[keyword] > 0:
                            idf = max(0.1, math.log((total_docs - keyword_df[keyword] + 0.5) / (keyword_df[keyword] + 0.5)))
                        else:
                            idf = 0.1
                        
                        for chunk_id in self.inverted_index[keyword]:
                            if chunk_id not in chunk_scores:
                                chunk_scores[chunk_id] = 0
                            
                            # ç®€å•çš„TF (è¯é¢‘) è®¡ç®—
                            chunk = self.chunk_mapping[chunk_id]
                            content = chunk.get('content', '').lower()
                            tf = content.count(keyword) / max(1, len(content.split()))
                            
                            # BM25å¯å‘çš„åˆ†æ•°è®¡ç®—
                            k1 = 1.2  # BM25å‚æ•°
                            b = 0.75  # BM25å‚æ•°
                            avg_doc_length = sum(len(chunk.get('content', '').split()) for chunk in self.chunk_mapping) / max(1, total_docs)
                            doc_length = len(content.split())
                            
                            # è®¡ç®—æœ€ç»ˆå¾—åˆ†
                            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_length / avg_doc_length))
                            chunk_scores[chunk_id] += score
                
                # 2. å…¨æ–‡æ¨¡ç³ŠåŒ¹é…ä½œä¸ºè¡¥å……ï¼ˆå¤„ç†æœªåœ¨å€’æ’ç´¢å¼•ä¸­çš„æƒ…å†µï¼‰
                query_lower = query.lower()
                for chunk_id, chunk in enumerate(self.chunk_mapping):
                    content = chunk.get('content', '').lower()
                    metadata = chunk.get('metadata', {})
                    filename = metadata.get('filename', '').lower()
                    
                    # è®¡ç®—å¤šç§åŒ¹é…æŒ‡æ ‡
                    content_score = 0
                    exact_matches = 0
                    partial_matches = 0
                    
                    # è®¡ç®—ç²¾ç¡®åŒ¹é…å’Œéƒ¨åˆ†åŒ¹é…
                    for keyword in keywords:
                        if keyword in content:
                            exact_matches += 1
                            # è®¡ç®—å…³é”®è¯åœ¨å†…å®¹ä¸­çš„ä½ç½®æƒé‡ï¼ˆå‰é¢çš„å…³é”®è¯æ›´é‡è¦ï¼‰
                            pos = content.find(keyword)
                            pos_weight = max(0.5, 1 - pos / max(1, len(content)))
                            content_score += 1.0 * pos_weight
                        elif any(keyword in word for word in content.split()):
                            partial_matches += 1
                            content_score += 0.3  # éƒ¨åˆ†åŒ¹é…æƒé‡è¾ƒä½
                    
                    # æ–‡ä»¶ååŒ¹é…ï¼ˆæƒé‡æ›´é«˜ï¼‰
                    filename_score = 0
                    for keyword in keywords:
                        if keyword in filename:
                            filename_score += 2.0  # æé«˜æ–‡ä»¶ååŒ¹é…æƒé‡
                    
                    # æ ‡é¢˜åŒ¹é…ï¼ˆå¦‚æœæœ‰ï¼‰
                    title = metadata.get('title', '').lower()
                    title_score = 0
                    for keyword in keywords:
                        if keyword in title:
                            title_score += 1.8  # æ ‡é¢˜åŒ¹é…æƒé‡
                    
                    # è®¡ç®—å†…å®¹ç›¸å…³æ€§ï¼ˆå…³é”®è¯è¦†ç›–ç‡ï¼‰
                    coverage = exact_matches / len(keywords)
                    
                    # ç»¼åˆåˆ†æ•°
                    total_score = (content_score * 0.5) + (filename_score * 0.3) + (title_score * 0.2)
                    
                    # åº”ç”¨è¦†ç›–ç‡æå‡
                    if coverage > 0:
                        total_score *= (1 + coverage * 0.5)
                    
                    if total_score > 0:
                        if chunk_id not in chunk_scores:
                            chunk_scores[chunk_id] = 0
                        chunk_scores[chunk_id] += total_score
                
                # ç”Ÿæˆç»“æœåˆ—è¡¨
                results = []
                for chunk_id, score in chunk_scores.items():
                    if score >= min_score:
                        try:
                            chunk = self.chunk_mapping[chunk_id]
                            content = chunk.get('content', 'å†…å®¹ä¸å¯ç”¨')
                            metadata = chunk.get('metadata', {})
                            
                            # è®¡ç®—é¢å¤–çš„ç›¸å…³æ€§æŒ‡æ ‡
                            relevance_score = 0
                            
                            # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼ˆé€‚ä¸­çš„å†…å®¹æ›´å¯èƒ½æ˜¯æœ‰ç”¨çš„ï¼‰
                            content_length = len(content)
                            if 50 <= content_length <= 500:
                                relevance_score += 0.2
                            elif content_length > 500:
                                relevance_score += 0.1
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªå…³é”®è¯
                            keyword_count = 0
                            for keyword in keywords:
                                if keyword in content.lower():
                                    keyword_count += 1
                            if keyword_count >= len(keywords) * 0.7:
                                relevance_score += 0.3
                            
                            # åº”ç”¨ç›¸å…³æ€§è°ƒæ•´
                            final_score = score * (1 + relevance_score)
                            
                            # å½’ä¸€åŒ–åˆ†æ•°ï¼ˆä½¿ç”¨logç¼©æ”¾é¿å…åˆ†æ•°è¿‡é«˜ï¼‰
                            normalized_score = min(1.0, math.log(final_score + 1) / 3)
                            
                            result = {
                                'content': content,
                                'metadata': metadata,
                                'score': normalized_score,
                                'is_keyword_match': True,
                                'keyword_count': keyword_count,
                                'coverage': keyword_count / len(keywords)
                            }
                            results.append(result)
                        except (IndexError, KeyError):
                            continue
                
                # é‡æ’åºï¼šç»¼åˆè€ƒè™‘åˆ†æ•°ã€å…³é”®è¯è¦†ç›–ç‡å’Œå†…å®¹è´¨é‡
                results.sort(key=lambda x: (x['score'], x['coverage']), reverse=True)
                
                # ç§»é™¤å†—ä½™ç»“æœï¼ˆé¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„å†…å®¹ï¼‰
                unique_results = []
                seen_contents = set()
                for result in results:
                    # æå–å†…å®¹æŒ‡çº¹ç”¨äºå»é‡
                    content_fingerprint = ' '.join(sorted([keyword for keyword in keywords if keyword in result['content'].lower()]))
                    content_preview = result['content'][:100]
                    fingerprint = f"{content_fingerprint}:{content_preview}"
                    
                    if fingerprint not in seen_contents:
                        seen_contents.add(fingerprint)
                        unique_results.append(result)
                        if len(unique_results) >= top_k:
                            break
                
                print(f"å¢å¼ºç‰ˆæœç´¢å®Œæˆï¼Œè¿”å› {len(unique_results)} ä¸ªç»“æœï¼Œå…³é”®è¯è¦†ç›–ç‡æœ€é«˜: {max([r['coverage'] for r in unique_results]) if unique_results else 0:.2f}")
                return unique_results
            
            def generate_rag_response(self, query, top_k=5):
                """
                ç”ŸæˆåŸºäºæ£€ç´¢ç»“æœçš„RAGå›ç­”
                å‚æ•°:
                    query: ç”¨æˆ·æŸ¥è¯¢
                    top_k: è¿”å›çš„ç»“æœæ•°é‡
                è¿”å›:
                    åŸºäºæ£€ç´¢ç»“æœçš„ç»¼åˆå›ç­”
                """
                # é¦–å…ˆæ‰§è¡Œæœç´¢è·å–ç›¸å…³æ–‡æ¡£
                search_results = self.search(query, top_k=top_k)
                
                if not search_results:
                    return f"é’ˆå¯¹'{query}'çš„é—®é¢˜ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯ã€‚"
                
                # æ–‡æœ¬æ¸…ç†å‡½æ•°
                def clean_text(text):
                    import re
                    if not text:
                        return ""
                    
                    # ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œä¹±ç 
                    cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿ.,!?:;ï¼›\s\(\)\[\]\{\}"\']', '', cleaned)
                    cleaned = re.sub(r'\s+', ' ', cleaned)
                    return cleaned.strip()
                
                # å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥è°ƒç”¨å¤§æ¨¡å‹è¿›è¡ŒRAGç”Ÿæˆ
                import os
                aliyun_api_key = os.getenv("ALIYUN_BAILIAN_API_KEY", "")
                
                # å¦‚æœæœ‰APIå¯†é’¥ï¼Œå°è¯•è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆRAGå›ç­”
                if aliyun_api_key:
                    try:
                        print("å°è¯•ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼Qwen-Turbo v1æ¨¡å‹ç”ŸæˆRAGå›ç­”...")
                        
                        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
                        context = ""
                        source_info = []
                        
                        for i, result in enumerate(search_results[:3], 1):  # ä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                            content = clean_text(result.get('content', ''))
                            metadata = result.get('metadata', {})
                            filename = metadata.get('filename', 'æœªçŸ¥æ¥æº')
                            
                            context += f"ã€ä¿¡æ¯æ¥æº{i}: {filename}ã€‘\n{content}\n\n"
                            source_info.append(filename)
                        
                        # æ„å»ºæç¤ºè¯
                        prompt = f"""ä¸Šä¸‹æ–‡æ„å»ºç­–ç•¥ï¼Œ 
 
 -ç²¾å‡†æ£€ç´¢ï¼šæ¯æ¬¡é—®ç­”å°†ä¸¥æ ¼ç­›é€‰å¹¶é‡‡ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„æ£€ç´¢ç»“æœä½œä¸ºä¿¡æ¯åŸºç¡€ï¼Œç¡®ä¿ç­”æ¡ˆçš„æ ¸å¿ƒæ€§ä¸å‡†ç¡®æ€§ã€‚ 
 
 -å†…å®¹æ¸…ç†ï¼šåœ¨æ„å»ºç­”æ¡ˆå‰ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¯¹åŸå§‹å†…å®¹è¿›è¡Œå‡€åŒ–ï¼Œå»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦ã€ä¹±ç åŠä¸ç›¸å…³çš„å†—ä½™æ•°æ®ï¼Œä¿è¯ä¿¡æ¯çš„çº¯å‡€ä¸å¯è¯»æ€§ã€‚ 
 
 
 è§’è‰²å®šä½ 
 
 ä½ æ˜¯ä¸€ä¸ªã€Œè¿ªå£«å°¼çŸ¥è¯†åº“åŠ©æ‰‹ã€æœ‰ä¸°å¯Œçš„å…³äºè¿ªå£«å°¼çš„èµ„æ–™å‚¨å¤‡ï¼Œä¹Ÿèƒ½æœç´¢æœ€æ–°çŠ¶æ€çš„æ–°é—»ï¼Œé¦–è¦æ ¹æ®çŸ¥è¯†åº“çš„å‚¨å¤‡ï¼Œæ¬¡è¦å’Œç½‘ç»œæœç´¢ç›¸ç»“åˆï¼Œæ€»ç»“å½’çº³ç”¨æˆ·çš„é—®é¢˜ã€‚ä¸å•ç‹¬æ˜¾ç¤ºèµ„æ–™æ¥æºã€‚ 
 
 -ä¿¡æ¯æ•´åˆåŸåˆ™ 
 æˆ‘çš„å›ç­”å°†é¦–è¦ä¾æ®å†…éƒ¨çŸ¥è¯†åº“çš„ä¸°å¯Œèµ„æ–™ã€‚å½“é‡åˆ°çŸ¥è¯†åº“ä¸­ä¿¡æ¯ä¸å®Œæ•´æˆ–å¯èƒ½è¿‡æ—¶çš„æƒ…å†µï¼Œæˆ‘ä¼šè°¨æ…åœ°ç»“åˆç½‘ç»œä¸Šçš„æƒå¨æ–°é—»è¿›è¡Œè¡¥å……å’Œæ›´æ–°ï¼Œç¡®ä¿æ‚¨è·å¾—çš„ä¿¡æ¯æ—¢å‡†ç¡®åˆå…¨é¢ã€‚ 
 
 -è§’è‰²ä¸è¯­è¨€é£æ ¼ 
 æˆ‘ä¼šå…¨ç¨‹æ‰®æ¼”å¥½"è¿ªå£«å°¼çŸ¥è¯†åº“åŠ©æ‰‹"çš„è§’è‰²ï¼Œä½¿ç”¨çƒ­æƒ…ã€äº²åˆ‡ä¸”å……æ»¡æ•…äº‹æ€§çš„è¯­è¨€ä¸æ‚¨äº¤æµï¼Œå°±åƒåœ¨è¿ªå£«å°¼ä¹å›­ä¸­ä¸æ‚¨å¯¹è¯ä¸€æ ·ï¼Œä¸ºæ‚¨è¥é€ æ²‰æµ¸å¼çš„ä½“éªŒã€‚ 
 
 -å†…å®¹å‘ˆç°æ–¹å¼ 
 åœ¨å›ç­”ä¸­ï¼Œæˆ‘ä¼šè‡ªç„¶åœ°èåˆä¿¡æ¯ï¼Œé¿å…ç®€å•ç½—åˆ—è¦ç‚¹ã€‚å¯¹äºéœ€è¦æ¨ç†æˆ–æ­¥éª¤è¯´æ˜çš„é—®é¢˜ï¼Œæˆ‘ä¼šé‡‡ç”¨å¾ªåºæ¸è¿›çš„è§£é‡Šæ–¹å¼ï¼Œè®©å¤æ‚çš„å†…å®¹ä¹Ÿå˜å¾—æ¸…æ™°æ˜“æ‡‚ã€‚æ‰€æœ‰ä¿¡æ¯æ¥æºéƒ½ä¼šæ— ç¼èå…¥å›ç­”ï¼Œä¸ä¼šå‡ºç°"æ ¹æ®èµ„æ–™æ˜¾ç¤º"è¿™ç±»ç”Ÿç¡¬çš„æœ¯è¯­ã€‚ 
 
 -è´¨é‡ä¿è¯ 
 ä¸ºäº†ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»“åˆç½‘ç»œä¿¡æ¯æ—¶ï¼Œæˆ‘ä¼šäº¤å‰éªŒè¯å¤šä¸ªæ¥æºï¼Œå¹¶ä¼˜å…ˆé‡‡çº³å®˜æ–¹åŠæƒå¨åª’ä½“å‘å¸ƒçš„ä¿¡æ¯ã€‚ 
 
 -ä¸¥è°¨èƒ¡ä¹±ç­”å¤ï¼šå¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æˆ–ä¿¡æ¯ä¸è¶³ï¼Œæˆ‘ä¼šå¦è¯šå‘ŠçŸ¥ï¼Œç»ä¸ä¼šéšæ„ç¼–é€ æˆ–æ¨æµ‹å†…å®¹ã€‚

é—®é¢˜: {query}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

è¯·å¼€å§‹å›ç­”:"""
                        
                        # è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
                        import requests
                        import json
                        
                        url = "https://bailian.aliyuncs.com/v1/chat/completions"
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {aliyun_api_key}"
                        }
                        
                        data = {
                            "model": "qwen-turbo",  # ä½¿ç”¨Qwen-Turbo v1æ¨¡å‹
                            "messages": [
                                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ªå£«å°¼çŸ¥è¯†åº“åŠ©æ‰‹ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3,
                            "max_tokens": 1000
                        }
                        
                        response = requests.post(url, headers=headers, json=data, timeout=30)
                        response.raise_for_status()
                        
                        # è§£æå“åº”
                        result = response.json()
                        print(f"è°ƒè¯•ä¿¡æ¯ï¼šé˜¿é‡Œäº‘APIå“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
                        
                        # å®‰å…¨åœ°è·å–å“åº”å†…å®¹
                        try:
                            if 'choices' in result and len(result['choices']) > 0:
                                if 'message' in result['choices'][0] and 'content' in result['choices'][0]['message']:
                                    rag_answer = result['choices'][0]['message']['content'].strip()
                                elif 'text' in result['choices'][0]:
                                    # æŸäº›æ¨¡å‹å¯èƒ½ä½¿ç”¨textå­—æ®µ
                                    rag_answer = result['choices'][0]['text'].strip()
                                else:
                                    raise KeyError("å“åº”æ ¼å¼ä¸åŒ…å«é¢„æœŸçš„æ¶ˆæ¯å†…å®¹å­—æ®µ")
                            else:
                                raise KeyError("å“åº”ä¸­æ²¡æœ‰choiceså­—æ®µæˆ–ä¸ºç©º")
                        except (KeyError, IndexError) as e:
                            print(f"âš ï¸ å“åº”æ ¼å¼è§£æé”™è¯¯: {str(e)}")
                            # å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½çš„å“åº”æ ¼å¼
                            if 'result' in result:
                                rag_answer = str(result['result']).strip()
                            elif 'content' in result:
                                rag_answer = str(result['content']).strip()
                            else:
                                raise KeyError("æ— æ³•ä»å“åº”ä¸­æå–å†…å®¹")
                        
                        # æ·»åŠ æ¥æºä¿¡æ¯
                        if source_info:
                            sources_text = "\n\nç›¸å…³ä¿¡æ¯æ¥æº: " + "ã€".join(source_info)
                            if len(rag_answer) + len(sources_text) < 600:
                                rag_answer += sources_text
                        
                        print("âœ“ æˆåŠŸä½¿ç”¨å¤§æ¨¡å‹ç”ŸæˆRAGå›ç­”")
                        return rag_answer
                        
                    except Exception as e:
                        print(f"âš ï¸ é˜¿é‡Œäº‘ç™¾ç‚¼APIè°ƒç”¨å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                        # å¦‚æœå¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿçš„æ–‡æœ¬æ‹¼æ¥æ–¹æ³•
                
                # ä¼ ç»Ÿæ–¹æ³•ï¼šå°†æ£€ç´¢åˆ°çš„å†…å®¹è¿›è¡Œæ‹¼æ¥å’Œç»„ç»‡
                # æ„å»ºå›ç­”
                answer = f"é’ˆå¯¹'{query}'çš„é—®é¢˜ï¼Œæ ¹æ®çŸ¥è¯†åº“ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†å›ç­”ï¼š\n\n"
                
                # æ·»åŠ æœç´¢ç»“æœä¸­çš„å…³é”®ä¿¡æ¯
                seen_info = set()  # ç”¨äºå»é‡
                info_count = 0
                max_info = 3  # æœ€å¤šä½¿ç”¨3ä¸ªç»“æœ
                
                for result in search_results[:max_info]:
                    content = result.get('content', '')
                    metadata = result.get('metadata', {})
                    filename = metadata.get('filename', 'æœªçŸ¥æ¥æº')
                    score = result.get('score', 0)
                    
                    # æ¸…ç†å†…å®¹
                    cleaned_content = clean_text(content)
                    
                    # å¦‚æœå†…å®¹å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡
                    if len(cleaned_content) < 20:
                        continue
                    
                    # æå–å…³é”®å¥å­
                    import re
                    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', cleaned_content)
                    relevant_sentences = []
                    
                    # åªä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„å¥å­
                    query_lower = query.lower()
                    for sentence in sentences:
                        sentence_lower = sentence.lower()
                        # æ£€æŸ¥å¥å­æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯æˆ–ç›¸å…³ä¿¡æ¯
                        if any(keyword in sentence_lower for keyword in query_lower.split()) or len(relevant_sentences) < 2:
                            if len(sentence.strip()) > 10:
                                relevant_sentences.append(sentence.strip())
                                if len(relevant_sentences) >= 2:
                                    break
                    
                    # æ·»åŠ åˆ°å›ç­”ä¸­
                    if relevant_sentences:
                        info_count += 1
                        answer += f"ã€{info_count}. {filename}ã€‘\n"
                        for sentence in relevant_sentences:
                            if sentence not in seen_info:
                                seen_info.add(sentence)
                                answer += f"{sentence}\n"
                        answer += "\n"
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ
                if info_count == 0 and search_results:
                    first_result = search_results[0]
                    content = clean_text(first_result.get('content', ''))
                    filename = first_result.get('metadata', {}).get('filename', 'æœªçŸ¥æ¥æº')
                    answer += f"ã€1. {filename}ã€‘\n"
                    answer += f"{content[:300]}...\n"
                
                # æ·»åŠ ç»“å°¾
                answer += "ä»¥ä¸Šä¿¡æ¯æ¥æºäºè¿ªå£«å°¼çŸ¥è¯†åº“ï¼Œå¸Œæœ›èƒ½å¸®åŠ©æ‚¨æ›´å¥½åœ°äº†è§£ç›¸å…³å†…å®¹ã€‚"
                
                return answer
        
        # ä¼˜å…ˆä½¿ç”¨ç®€åŒ–ç‰ˆåŠ©æ‰‹ï¼Œé¿å…ç½‘ç»œä¾èµ–
        print("ä¼˜å…ˆä½¿ç”¨ç®€åŒ–ç‰ˆè¿ªå£«å°¼RAGåŠ©æ‰‹...")
        
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        chunk_mapping_path = global_os.path.join(index_path, 'chunk_mapping.json')
        simple_index_path = global_os.path.join(index_path, 'simple_index.json')
        if global_os.path.exists(chunk_mapping_path) and global_os.path.exists(simple_index_path):
            print(f"ç´¢å¼•æ–‡ä»¶å­˜åœ¨: {chunk_mapping_path}")
            print(f"ç´¢å¼•æ–‡ä»¶å­˜åœ¨: {simple_index_path}")
        else:
            print(f"è­¦å‘Š: ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´")
            print(f"chunk_mapping.json å­˜åœ¨: {global_os.path.exists(chunk_mapping_path)}")
            print(f"simple_index.json å­˜åœ¨: {global_os.path.exists(simple_index_path)}")
        
        # ç›´æ¥ä½¿ç”¨ç®€åŒ–ç‰ˆåŠ©æ‰‹
        try:
            rag_assistant = SimpleDisneyAssistant(index_dir=index_path)
            load_success = rag_assistant.load_index()
            
            if load_success:
                print("âœ… ç®€åŒ–ç‰ˆRAGåŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ")
                print("ğŸ“‹ å½“å‰åŠŸèƒ½çŠ¶æ€:")
                print("  - å…³é”®è¯æœç´¢: å·²å¯ç”¨")
                print("  - å‘é‡æœç´¢: ä¸æ”¯æŒ")
                print("  - RAGç”Ÿæˆ: åŸºäºå…³é”®è¯æœç´¢ç»“æœ")
                return True
            else:
                print("âŒ ç®€åŒ–ç‰ˆåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ åˆ›å»ºç®€åŒ–ç‰ˆåŠ©æ‰‹å¤±è´¥: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return False

# é¦–é¡µè·¯ç”±
@app.route('/')
def index():
    return render_template('index.html')

# æœç´¢API
@app.route('/api/search', methods=['POST'])
def search():
    if not rag_assistant:
        return jsonify({
            'success': False,
            'error': 'åŠ©æ‰‹å°šæœªåˆå§‹åŒ–'
        }), 503
    
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        use_vector_search = data.get('use_vector_search', True)  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦ä½¿ç”¨å‘é‡æœç´¢
        top_k = data.get('top_k', 5)  # æ–°å¢å‚æ•°ï¼šè¿”å›ç»“æœæ•°é‡
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ‰§è¡Œæœç´¢
        print(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query}, å‘é‡æœç´¢: {use_vector_search}")
        results = rag_assistant.search(query, top_k=top_k, use_vector_search=use_vector_search)
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºNone
        if results is None:
            results = []
            print("æ³¨æ„ï¼šæœç´¢è¿”å›Noneï¼Œå·²è½¬æ¢ä¸ºç©ºåˆ—è¡¨")
        
        print(f"æœç´¢ç»“æœæ•°é‡: {len(results)}")
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for i, result in enumerate(results):
            print(f"ç»“æœ {i+1}: {result.keys()}")
            # ç¡®ä¿ç»“æœä¸­åŒ…å«å¿…è¦çš„é”®
            metadata = result.get('metadata', {})
            filename = metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            content = result.get('content', 'å†…å®¹ä¸å¯ç”¨')
            score = result.get('score', 0)
            
            # æå–åˆ†ç±»ä¿¡æ¯ï¼ˆä»æ–‡ä»¶åï¼‰
            category = 'é€šç”¨'
            
            # æ ¹æ®æ–‡ä»¶åç®€å•åˆ†ç±»
            if 'é—¨ç¥¨' in filename or 'ç¥¨åŠ¡' in filename:
                category = 'é—¨ç¥¨ä¿¡æ¯'
            elif 'é…’åº—' in filename:
                category = 'é…’åº—æœåŠ¡'
            elif 'é¡¹ç›®' in filename or 'æ¸¸ç©' in filename:
                category = 'æ¸¸ä¹é¡¹ç›®'
            elif 'é¤é¥®' in filename:
                category = 'é¤é¥®æœåŠ¡'
            elif 'ä¼šå‘˜' in filename or 'å°Šäº«' in filename:
                category = 'ä¼šå‘˜æœåŠ¡'
            elif 'åœ°å›¾' in filename or 'åŒºåŸŸ' in filename:
                category = 'å›­åŒºå¯¼è§ˆ'
            elif 'æ”»ç•¥' in filename:
                category = 'æ¸¸ç©æ”»ç•¥'
            
            # æ·»åŠ æœç´¢ç±»å‹æ ‡è®°
            search_type = 'æ··åˆæ£€ç´¢'
            if 'is_keyword_match' in result and not use_vector_search:
                search_type = 'å…³é”®è¯æ£€ç´¢'
            elif 'distance' in result:
                search_type = 'å‘é‡æ£€ç´¢'
            
            formatted_results.append({
                'score': f"{score:.4f}",
                'filename': filename,
                'content': content,
                'category': category,
                'search_type': search_type
            })
        
        return jsonify({
            'success': True,
            'results': formatted_results,
            'search_type': 'æ··åˆæ£€ç´¢' if use_vector_search else 'å…³é”®è¯æ£€ç´¢'
        })
        
    except Exception as e:
        print(f"æœç´¢é”™è¯¯è¯¦ç»†ä¿¡æ¯: {str(e)}")
        import traceback
        print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# RAGç”ŸæˆAPI - éšè—AIç›¸å…³æè¿°
@app.route('/api/generate', methods=['POST'])
def generate_rag():
    """ç”ŸæˆçŸ¥è¯†åº“å›ç­”"""
    global rag_assistant  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    if not rag_assistant:
        return jsonify({
            'success': False,
            'error': 'åŠ©æ‰‹å°šæœªåˆå§‹åŒ–'
        }), 503
    
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        top_k = data.get('top_k', 5)  # å¢åŠ é»˜è®¤è¿”å›çš„ç»“æœæ•°é‡
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º'
            }), 400
        
        print(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query}")
        
        # é¦–å…ˆæ£€æŸ¥ç´¢å¼•æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        if not hasattr(rag_assistant, 'initialized') or not rag_assistant.initialized:
            # å°è¯•é‡æ–°åŠ è½½ç´¢å¼•
            print("ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åŠ è½½...")
            rag_assistant.load_index()
            if not rag_assistant.initialized:
                return jsonify({
                    'success': False,
                    'error': 'çŸ¥è¯†åº“ç´¢å¼•åŠ è½½å¤±è´¥',
                    'note': 'è¯·æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨'
                })
        
        # æ‰§è¡Œæœç´¢
        results = rag_assistant.search(query, top_k=top_k)
        
        # å³ä½¿æœç´¢ç»“æœä¸ºç©ºï¼Œä¹Ÿç»§ç»­æ‰§è¡Œç”Ÿæˆè¿‡ç¨‹
        # è¿™æ ·æˆ‘ä»¬çš„generate_rag_responseæ–¹æ³•ä»ç„¶ä¼šè¢«è°ƒç”¨
        if not results:
            print("âš ï¸ æœç´¢ç»“æœä¸ºç©ºï¼Œä½†ä»ä¼šå°è¯•ç”Ÿæˆå›ç­”")
            # åˆ›å»ºç©ºçš„formatted_resultsï¼Œå…è®¸åç»­æµç¨‹ç»§ç»­
            formatted_results = []
        
        # åªæœ‰åœ¨resultsä¸ä¸ºç©ºæ—¶æ‰æ ¼å¼åŒ–æœç´¢ç»“æœ
        if results and 'formatted_results' not in locals():
            formatted_results = []
            for result in results:
                metadata = result.get('metadata', {})
                filename = metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                content = result.get('content', 'å†…å®¹ä¸å¯ç”¨')
                score = result.get('score', 0)
                
                # æå–åˆ†ç±»ä¿¡æ¯
                category = 'é€šç”¨'
                if 'é—¨ç¥¨' in filename or 'ç¥¨åŠ¡' in filename:
                    category = 'é—¨ç¥¨ä¿¡æ¯'
                elif 'é…’åº—' in filename:
                    category = 'é…’åº—æœåŠ¡'
                elif 'é¡¹ç›®' in filename or 'æ¸¸ç©' in filename:
                    category = 'æ¸¸ä¹é¡¹ç›®'
                elif 'é¤é¥®' in filename:
                    category = 'é¤é¥®æœåŠ¡'
                elif 'ä¼šå‘˜' in filename or 'å°Šäº«' in filename:
                    category = 'ä¼šå‘˜æœåŠ¡'
                elif 'åœ°å›¾' in filename or 'åŒºåŸŸ' in filename:
                    category = 'å›­åŒºå¯¼è§ˆ'
                elif 'æ”»ç•¥' in filename:
                    category = 'æ¸¸ç©æ”»ç•¥'
                
                formatted_results.append({
                    'filename': filename,
                    'content': content,
                    'score': score,
                    'category': category
                })
        
        # ä¼˜åŒ–çš„ç®€æ´æ€»ç»“ç”Ÿæˆï¼Œæ›´å¥½åœ°æç‚¼ç”¨æˆ·é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
        def generate_brief_summary(query, results):
            """ç”Ÿæˆä¸è¶…è¿‡600å­—çš„è¯¦ç»†æ€»ç»“ï¼Œå‡†ç¡®æç‚¼ç”¨æˆ·é—®é¢˜å¹¶æä¾›ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆ"""
            # æ–‡æœ¬æ¸…ç†å‡½æ•°ï¼Œå¢å¼ºä¹±ç å¤„ç†èƒ½åŠ›
            def clean_text(text):
                if not text:
                    return ""
                
                # ç¬¬ä¸€æ­¥ï¼šç§»é™¤æ§åˆ¶å­—ç¬¦å’Œä¸å¯è§å­—ç¬¦
                cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                
                # ç¬¬äºŒæ­¥ï¼šå¤„ç†å¯èƒ½çš„ç¼–ç æ··åˆé—®é¢˜
                # ç§»é™¤æ˜æ˜¾çš„ä¹±ç æ¨¡å¼ï¼ˆè¿ç»­çš„éä¸­æ–‡å­—ç¬¦ä¸”ä¸æ˜¯è‹±æ–‡/æ•°å­—ï¼‰
                # ä¿ç•™åŸºæœ¬çš„ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸è§æ ‡ç‚¹
                cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿ.,!?:;ï¼›\s\(\)\[\]\{\}\"\']', '', cleaned)
                
                # ç¬¬ä¸‰æ­¥ï¼šç§»é™¤è¿ç»­çš„ç©ºç™½å­—ç¬¦
                cleaned = re.sub(r'\s+', ' ', cleaned)
                
                # ç¬¬å››æ­¥ï¼šç§»é™¤è¿ç»­çš„æ ‡ç‚¹ç¬¦å·
                cleaned = re.sub(r'(ï¼Œ|ã€‚|ï¼|ï¼Ÿ|,|\.|!|\?|:|ï¼›|;)\1+', '\1', cleaned)
                
                # ç¬¬äº”æ­¥ï¼šå¤„ç†å¸¸è§çš„ç¼–ç é”™è¯¯æ¨¡å¼
                # ç§»é™¤æ˜æ˜¾çš„ä¹±ç å­—ç¬¦ç»„ï¼ˆ5ä¸ªä»¥ä¸Šçš„è¿ç»­éä¸­æ–‡/éè‹±æ–‡/éæ•°å­—å­—ç¬¦ï¼‰
                cleaned = re.sub(r'([^\u4e00-\u9fa5a-zA-Z0-9\s]){5,}', '', cleaned)
                
                return cleaned.strip()
            
            # åˆ†æç”¨æˆ·é—®é¢˜ç±»å‹ï¼Œæå–æ ¸å¿ƒé—®é¢˜
            def analyze_query(query):
                # æ ¹æ®å¸¸è§é—®é¢˜ç±»å‹è¿›è¡Œåˆ†ç±»
                if any(word in query for word in ['å“ªé‡Œ', 'ä½ç½®', 'åœ°å›¾', 'è·¯çº¿']):
                    return 'ä½ç½®æŸ¥è¯¢', 'ä½ç½®ä¿¡æ¯'
                elif any(word in query for word in ['æ—¶é—´', 'å¼€æ”¾', 'é—­å›­', 'è¡¨æ¼”', 'çƒŸèŠ±', 'å·¡æ¸¸']):
                    return 'æ—¶é—´æŸ¥è¯¢', 'æ—¶é—´å®‰æ’'
                elif any(word in query for word in ['ç¥¨ä»·', 'é—¨ç¥¨', 'ä»·æ ¼', 'å¤šå°‘é’±']):
                    return 'ä»·æ ¼æŸ¥è¯¢', 'ç¥¨ä»·ä¿¡æ¯'
                elif any(word in query for word in ['é¡¹ç›®', 'æ¸¸ç©', 'å¿…ç©', 'åˆºæ¿€', 'é€‚åˆ']):
                    return 'é¡¹ç›®æŸ¥è¯¢', 'æ¨èé¡¹ç›®'
                elif any(word in query for word in ['é¤é¥®', 'é¤å…', 'åƒ', 'é£Ÿç‰©']):
                    return 'é¤é¥®æŸ¥è¯¢', 'æ¨èé¤å…'
                elif any(word in query for word in ['é…’åº—', 'ä½å®¿', 'æˆ¿é—´']):
                    return 'ä½å®¿æŸ¥è¯¢', 'é…’åº—ä¿¡æ¯'
                elif any(word in query for word in ['æ”»ç•¥', 'å»ºè®®', 'æç¤º', 'æŠ€å·§']):
                    return 'æ”»ç•¥å’¨è¯¢', 'å®ç”¨å»ºè®®'
                else:
                    return 'ä¸€èˆ¬å’¨è¯¢', 'ç›¸å…³ä¿¡æ¯'
            
            # æå–é‡è¦çš„ä¿¡æ¯ç‰‡æ®µï¼ŒæŒ‰é—®é¢˜ç±»å‹å’Œè§£å†³æ–¹æ¡ˆåˆ†ç±»
            problem_type, solution_category = analyze_query(query)
            core_answers = []  # æ ¸å¿ƒè§£å†³æ–¹æ¡ˆ
            supplementary_info = []  # è¡¥å……ä¿¡æ¯
            max_length = 550  # é¢„ç•™ç»“å°¾ç©ºé—´
            
            # æŒ‰ç›¸å…³æ€§ä¼˜å…ˆå¤„ç†ç»“æœ
            for result in results[:3]:  # åªå¤„ç†å‰3ä¸ªç»“æœ
                raw_content = result.get('content', '').strip()
                content = clean_text(raw_content)
                
                # æå–çŸ­å¥å¹¶æ¸…ç†
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', content)
                sentences = [clean_text(s) for s in sentences if len(clean_text(s)) > 5]
                
                # æ›´ç²¾ç¡®çš„å…³é”®è¯åˆ†æï¼ŒåŒºåˆ†é—®é¢˜å…³é”®è¯å’Œè§£å†³æ–¹æ¡ˆå…³é”®è¯
                query_lower = query.lower()
                # è§£å†³æ–¹æ¡ˆå…³é”®è¯åˆ—è¡¨
                solution_keywords = {
                    'ä½ç½®æŸ¥è¯¢': ['ä½äº', 'åœ¨', 'åœ°å€', 'åœ°å›¾', 'æ–¹å‘'],
                    'æ—¶é—´æŸ¥è¯¢': ['å¼€æ”¾', 'é—­å›­', 'å¼€å§‹', 'ç»“æŸ', 'æ—¶é—´', 'å‡ ç‚¹', 'åˆ†é’Ÿ', 'å°æ—¶'],
                    'ä»·æ ¼æŸ¥è¯¢': ['å…ƒ', 'ä»·æ ¼', 'ç¥¨ä»·', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'å…è´¹'],
                    'é¡¹ç›®æŸ¥è¯¢': ['é¡¹ç›®', 'æ¸¸ç©', 'è®¾æ–½', 'ä½“éªŒ', 'èº«é«˜', 'å¹´é¾„'],
                    'é¤é¥®æŸ¥è¯¢': ['é¤å…', 'é£Ÿç‰©', 'å¥—é¤', 'ä»·æ ¼', 'æ¨è'],
                    'ä½å®¿æŸ¥è¯¢': ['é…’åº—', 'æˆ¿é—´', 'å…¥ä½', 'é€€æˆ¿', 'ä»·æ ¼'],
                    'æ”»ç•¥å’¨è¯¢': ['å»ºè®®', 'æç¤º', 'æŠ€å·§', 'æ¨è', 'æ³¨æ„']
                }
                
                # è·å–å½“å‰é—®é¢˜ç±»å‹çš„è§£å†³æ–¹æ¡ˆå…³é”®è¯
                current_solution_keywords = solution_keywords.get(problem_type, [])
                
                for sentence in sentences:
                    sentence_lower = sentence.lower()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜æ ¸å¿ƒè¯æ±‡æˆ–è§£å†³æ–¹æ¡ˆå…³é”®è¯
                    contains_query_core = any(keyword in sentence_lower for keyword in query_lower.split())
                    contains_solution_keyword = any(keyword in sentence_lower for keyword in current_solution_keywords)
                    has_specific_info = any(char.isdigit() for char in sentence) or any(word in sentence for word in ['æ˜¯', 'ä½äº', 'æä¾›', 'å¼€æ”¾'])
                    
                    # ä¼˜å…ˆé€‰æ‹©è§£å†³æ–¹æ¡ˆç±»ä¿¡æ¯
                    if contains_solution_keyword and has_specific_info:
                        if len(sentence) < 250 and len(''.join(core_answers)) + len(sentence) < max_length * 0.8:
                            core_answers.append(sentence)
                    # è¡¥å……ä¿¡æ¯æ¬¡ä¹‹
                    elif contains_query_core and not contains_solution_keyword:
                        if len(sentence) < 200 and len(''.join(core_answers + supplementary_info)) + len(sentence) < max_length:
                            supplementary_info.append(sentence)
            
            # æ„å»ºç»“æ„åŒ–å›ç­”
            summary = f"é’ˆå¯¹'{query}'çš„é—®é¢˜ï¼Œ"
            
            # ä¼˜å…ˆæ·»åŠ æ ¸å¿ƒè§£å†³æ–¹æ¡ˆ
            if core_answers:
                summary += f"ä»¥ä¸‹æ˜¯{solution_category}ï¼š"
                # æ·»åŠ ç¬¬ä¸€ä¸ªæ ¸å¿ƒè§£å†³æ–¹æ¡ˆ
                summary += core_answers[0]
                # æ·»åŠ å…¶ä»–æ ¸å¿ƒè§£å†³æ–¹æ¡ˆ
                for i, answer in enumerate(core_answers[1:], 1):
                    connector = 'å¦å¤–ï¼Œ' if i == 1 else 'è¿˜æœ‰ï¼Œ'
                    if len(summary + connector + answer) <= max_length:
                        summary += connector + answer
                    else:
                        break
            
            # é€‚å½“æ·»åŠ è¡¥å……ä¿¡æ¯
            if supplementary_info and len(summary) < max_length * 0.9:
                for info in supplementary_info:
                    connector = 'è¡¥å……è¯´æ˜ï¼Œ' if not core_answers else 'éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œ'
                    if len(summary + connector + info) <= max_length:
                        summary += connector + info
                    else:
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä¿¡æ¯ï¼Œæä¾›é»˜è®¤å›ç­”
            if len(summary) <= len(f"é’ˆå¯¹'{query}'çš„é—®é¢˜ï¼Œ") + len(f"ä»¥ä¸‹æ˜¯{solution_category}ï¼š"):
                if results and 'content' in results[0]:
                    first_content = clean_text(results[0]['content'][:max_length - len(summary) - 5])
                    summary += f"æˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸å…³ä¿¡æ¯ï¼š{first_content}..."
                else:
                    summary += "æˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸å…³ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹å‚è€ƒæ¥æºè·å–è¯¦æƒ…ã€‚"
            
            # æœ€ç»ˆæ¸…ç†å¹¶ç¡®ä¿æ€»é•¿åº¦ä¸è¶…è¿‡600å­—
            summary = clean_text(summary)
            if len(summary) > 600:
                summary = summary[:597] + "..."
            
            return summary
        
        try:
            # å°è¯•ä½¿ç”¨DisneyRAGAssistantçš„generate_rag_responseæ–¹æ³•ç”ŸæˆçœŸæ­£çš„RAGæŠ¥å‘Š
            print(f"ğŸ“ ä½¿ç”¨RAGç”Ÿæˆå®Œæ•´å›ç­”: {query}")
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹åˆ—è¡¨
            docs = []
            for result in formatted_results:
                if isinstance(result, dict) and 'content' in result:
                    docs.append(result['content'])
            
            # è°ƒç”¨generate_rag_responseæ–¹æ³•ç”Ÿæˆå®Œæ•´RAGæŠ¥å‘Š
            if hasattr(rag_assistant, 'generate_rag_response'):
                try:
                    print(f"å°è¯•ä½¿ç”¨generate_rag_responseæ–¹æ³•ç”Ÿæˆå›ç­”...")
                    # æ­£ç¡®è°ƒç”¨RAGç”Ÿæˆæ–¹æ³•ï¼Œåªä¼ å…¥queryå‚æ•°
                    rag_response = rag_assistant.generate_rag_response(query, top_k=5)
                    if rag_response and rag_response.get('success'):
                        print("âœ“ ä½¿ç”¨generate_rag_responseç”Ÿæˆå›ç­”æˆåŠŸ")
                        return jsonify({
                            'success': True,
                            'answer': rag_response.get('answer', ''),
                            'sources': rag_response.get('sources', []),
                            'model_used': rag_response.get('model_used', ''),
                            'fallback_results': formatted_results,
                            'note': 'ä½¿ç”¨generate_rag_responseæ–¹æ³•ç”Ÿæˆ',
                            'rag_type': 'full_rag'
                        })
                    else:
                        print("âš ï¸ RAGç”Ÿæˆè¿”å›ç©ºç»“æœï¼Œç›´æ¥è¿”å›æç¤º")
                        # å…³åœå›é€€æœºåˆ¶ï¼Œè¿”å›å›ºå®šæç¤ºè¯­
                        return jsonify({
                            'success': True,
                            'answer': 'å¯¹ä¸èµ·ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰åŠæ³•å¸®æ‚¨è§£å†³æ‚¨çš„é—®é¢˜ã€‚',
                            'fallback_results': formatted_results,
                            'note': 'RAGç”Ÿæˆå¤±è´¥ï¼Œå·²å…³åœå›é€€æœºåˆ¶',
                            'rag_type': 'direct_message'
                        })
                except Exception as rag_error:
                    print(f"âœ— RAGç”Ÿæˆé”™è¯¯: {str(rag_error)}")
                    # å…³åœå›é€€æœºåˆ¶ï¼Œè¿”å›å›ºå®šæç¤ºè¯­
                    return jsonify({
                        'success': True,
                        'answer': 'å¯¹ä¸èµ·ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰åŠæ³•å¸®æ‚¨è§£å†³æ‚¨çš„é—®é¢˜ã€‚',
                        'fallback_results': formatted_results,
                        'note': f'RAGç”Ÿæˆå¤±è´¥ï¼Œå·²å…³åœå›é€€æœºåˆ¶: {str(rag_error)}',
                        'rag_type': 'direct_message'
                    })
            else:
                print("âš ï¸ RAGåŠ©æ‰‹æ²¡æœ‰generate_rag_responseæ–¹æ³•ï¼Œç›´æ¥è¿”å›æç¤º")
                # å…³åœå›é€€æœºåˆ¶ï¼Œè¿”å›å›ºå®šæç¤ºè¯­
                return jsonify({
                    'success': True,
                    'answer': 'å¯¹ä¸èµ·ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰åŠæ³•å¸®æ‚¨è§£å†³æ‚¨çš„é—®é¢˜ã€‚',
                    'fallback_results': formatted_results,
                    'note': 'RAGåŠ©æ‰‹æ²¡æœ‰generate_rag_responseæ–¹æ³•ï¼Œå·²å…³åœå›é€€æœºåˆ¶',
                    'rag_type': 'direct_message'
                })
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            # è¿”å›åŸå§‹RAGç»“æœä½œä¸ºå¤‡é€‰
            # ä½¿ç”¨generate_brief_summaryå‡½æ•°ç”ŸæˆåŸºäºæœç´¢ç»“æœçš„æ‘˜è¦
            try:
                fallback_answer = generate_brief_summary(query, formatted_results)
                return jsonify({
                    'success': True,
                    'answer': fallback_answer,
                    'fallback_results': formatted_results,
                    'note': f'RAGç”Ÿæˆå‡ºé”™ï¼Œè¿”å›å¤‡é€‰æ‘˜è¦: {str(e)}',
                    'rag_type': 'fallback_summary'
                })
            except:
                # å¦‚æœæ‘˜è¦ç”Ÿæˆä¹Ÿå¤±è´¥ï¼Œè¿”å›åŸºç¡€ä¿¡æ¯
                return jsonify({
                    'success': True,
                    'answer': f"å·²ä¸ºæ‚¨æ‰¾åˆ°{len(formatted_results)}æ¡ç›¸å…³ä¿¡æ¯ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹ä¸‹æ–¹å‚è€ƒæ¥æºã€‚",
                    'fallback_results': formatted_results,
                    'note': 'RAGç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆéƒ½å¤±è´¥',
                    'rag_type': 'basic_info'
                })
        
    except Exception as e:
        print(f"æœç´¢é”™è¯¯: {str(e)}")
        import traceback
        print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        # æ•è·ç‰¹å®šé”™è¯¯å¹¶è¿”å›å‹å¥½ä¿¡æ¯
        error_msg = str(e)
        if 'APIå¯†é’¥' in error_msg or 'token' in error_msg.lower() or 'è®¤è¯' in error_msg:
            return jsonify({
                'success': False,
                'error': 'ç³»ç»Ÿç¼ºå°‘å¿…è¦çš„APIå¯†é’¥ï¼Œè¯·é…ç½®ç¯å¢ƒå˜é‡DASHSCOPE_API_KEYæˆ–ALIYUN_API_KEY'
            })
        elif 'DeepSeek' in error_msg:
            return jsonify({
                'success': False,
                'error': 'DeepSeek APIé”™è¯¯ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®'
            })
        elif 'è¯·å…ˆåŠ è½½ç´¢å¼•' in error_msg:
            return jsonify({
                'success': False,
                'error': 'çŸ¥è¯†åº“ç´¢å¼•æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {error_msg}',
                'note': 'è¯·å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯æˆ–ç¨åå†è¯•'
            })

# é…ç½®ç®¡ç†API
@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯"""
    # æ£€æµ‹æ˜¯å¦æœ‰é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥ç¯å¢ƒå˜é‡
    has_api_key = ('DASHSCOPE_API_KEY' in os.environ and os.environ['DASHSCOPE_API_KEY']) or \
                 ('ALIYUN_API_KEY' in os.environ and os.environ['ALIYUN_API_KEY'])
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å®Œæ•´ç‰ˆåŠ©æ‰‹
    is_full_version = hasattr(rag_assistant, 'generate_rag_response') if rag_assistant else False
    
    return jsonify({
        'success': True,
        'features': {
            'vector_search': is_full_version,  # å¦‚æœæ˜¯å®Œæ•´ç‰ˆåˆ™æ”¯æŒå‘é‡æœç´¢
            'rag_generation': has_api_key,  # åªæœ‰æœ‰APIå¯†é’¥æ—¶æ‰æ”¯æŒç”Ÿæˆ
            'keyword_search': True,
            'rerank': is_full_version  # å¦‚æœæ˜¯å®Œæ•´ç‰ˆåˆ™æ”¯æŒé‡æ’åº
        },
        'model_info': None,
        'environment': {
            'has_api_key': has_api_key,
            'mode': 'full' if is_full_version else 'simplified'  # å½“å‰è¿è¡Œæ¨¡å¼
        }
    })

# ç»Ÿè®¡ä¿¡æ¯API
@app.route('/api/stats', methods=['GET'])
def stats():
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    global rag_assistant  # å…¨å±€å˜é‡å£°æ˜å¿…é¡»åœ¨ä½¿ç”¨å‰
    try:
        # æ£€æŸ¥åŠ©æ‰‹æ˜¯å¦å·²åˆå§‹åŒ–
        if not rag_assistant:
            return jsonify({
                'success': False,
                'error': 'åŠ©æ‰‹å°šæœªåˆå§‹åŒ–'
            }), 503
        
        print("å¼€å§‹è·å–ç»Ÿè®¡ä¿¡æ¯...")
        
        # ä½¿ç”¨æ­£ç¡®çš„å±æ€§åç§°
        total_chunks = len(rag_assistant.chunk_mapping)
        total_index_words = len(rag_assistant.inverted_index)
        
        print(f"ç»Ÿè®¡ä¿¡æ¯: æ€»åˆ‡ç‰‡æ•°={total_chunks}, æ€»ç´¢å¼•è¯={total_index_words}")
        
        # ç»Ÿè®¡åˆ†ç±»ä¿¡æ¯
        categories = {}
        
        # éå†chunk_mappingæ¥è·å–åˆ†ç±»ä¿¡æ¯
        for chunk in rag_assistant.chunk_mapping:
            metadata = chunk.get('metadata', {})
            
            # å°è¯•ä»metadataä¸­è·å–åˆ†ç±»æˆ–æ–‡ä»¶å
            category = 'é€šç”¨'
            
            # ä¼˜å…ˆä»metadataè·å–filename
            if isinstance(metadata, dict):
                filename = metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                
                # åŸºäºæ–‡ä»¶åè¿›è¡Œåˆ†ç±»
                if 'é—¨ç¥¨' in filename or 'ç¥¨åŠ¡' in filename:
                    category = 'é—¨ç¥¨ä¿¡æ¯'
                elif 'é…’åº—' in filename:
                    category = 'é…’åº—æœåŠ¡'
                elif 'é¡¹ç›®' in filename or 'æ¸¸ç©' in filename:
                    category = 'æ¸¸ä¹é¡¹ç›®'
                elif 'é¤é¥®' in filename:
                    category = 'é¤é¥®æœåŠ¡'
                elif 'ä¼šå‘˜' in filename or 'å°Šäº«' in filename:
                    category = 'ä¼šå‘˜æœåŠ¡'
                elif 'åœ°å›¾' in filename or 'åŒºåŸŸ' in filename:
                    category = 'å›­åŒºå¯¼è§ˆ'
                elif 'æ”»ç•¥' in filename:
                    category = 'æ¸¸ç©æ”»ç•¥'
            
            categories[category] = categories.get(category, 0) + 1
        
        print(f"åˆ†ç±»ç»Ÿè®¡ç»“æœ: {categories}")
        
        # çƒ­é—¨æœç´¢è¯
        popular_searches = [
            'è¿ªå£«å°¼ä¹å›­é—¨ç¥¨', 
            'ä¼šå‘˜æƒç›Š', 
            'å•†å“é€€æ¢æ”¿ç­–', 
            'æœåŠ¡æ—¶é—´', 
            'å„¿ç«¥é¡¹ç›®'
        ]
        
        return jsonify({
            'success': True,
            'stats': {
                'total_chunks': total_chunks,
                'total_index_words': total_index_words,
                'categories': categories
            },
            'popular_searches': popular_searches
        })
    except Exception as e:
        print(f"ç»Ÿè®¡APIé”™è¯¯: {str(e)}")
        import traceback
        print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f'è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}'
        }), 500

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨ï¼ˆé€šè¿‡setup.pyä¸­çš„entry_pointsé…ç½®ï¼‰
    """
    print("åˆå§‹åŒ–è¿ªå£«å°¼RAGåŠ©æ‰‹...")
    
    init_success = init_assistant()
    if init_success:
        print("âœ… RAGåŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âš ï¸ RAGåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä»¥æœ‰é™åŠŸèƒ½è¿è¡Œ")

    print("å¯åŠ¨FlaskæœåŠ¡å™¨...")
    app.run(host='0.0.0.0', port=5000, debug=False)

if __name__ == '__main__':
    main()