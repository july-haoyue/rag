#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿ªå£«å°¼RAGæ£€ç´¢åŠ©æ‰‹ - FAISSå‘é‡ç‰ˆï¼ˆç®€åŒ–ç‰ˆï¼‰
é›†æˆFAISSå‘é‡åº“å’Œé˜¿é‡Œäº‘ç™¾ç‚¼APIï¼Œå®ç°æ£€ç´¢å’ŒRAGç”Ÿæˆ
"""

# åº”ç”¨huggingface_hubè¡¥ä¸ï¼Œè§£å†³ç¼ºå¤±split_torch_state_dict_into_shardså‡½æ•°çš„é—®é¢˜
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from patch_huggingface_hub import apply_patch
    apply_patch()
    print("âœ… å·²åº”ç”¨huggingface_hubè¡¥ä¸")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•åº”ç”¨huggingface_hubè¡¥ä¸: {e}")

import re
import json
import time
import numpy as np
import faiss
from typing import List, Dict, Any, Tuple

# å¯¼å…¥æ··åˆæ£€ç´¢ç›¸å…³æ¨¡å—
try:
    from hybrid_retriever import HybridRetriever
    from web_searcher import WebSearcher
    # retrieval_evaluatorè®¾ä¸ºå¯é€‰å¯¼å…¥
    try:
        from retrieval_evaluator import RetrievalEvaluator
        RETRIEVAL_EVALUATOR_AVAILABLE = True
    except ImportError:
        print("âš ï¸ retrieval_evaluatoræ¨¡å—ç¼ºå¤±ï¼Œå°†ä¸ä½¿ç”¨æ£€ç´¢è¯„ä¼°åŠŸèƒ½")
        RETRIEVAL_EVALUATOR_AVAILABLE = False
    HAS_HYBRID_RETRIEVAL = True
    print("âœ… æ··åˆæ£€ç´¢æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ æ··åˆæ£€ç´¢æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    HAS_HYBRID_RETRIEVAL = False
    RETRIEVAL_EVALUATOR_AVAILABLE = False

# è®¾ç½®æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.cache', 'huggingface')
os.makedirs(cache_dir, exist_ok=True)
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['SENTENCE_TRANSFORMERS_HOME'] = cache_dir

# å¯¼å…¥å¿…è¦çš„åº“
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    from openai import OpenAI
    # å°è¯•åŠ è½½.envæ–‡ä»¶
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass  # .envæ–‡ä»¶æ”¯æŒä¸ºå¯é€‰
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥åº“å¤±è´¥: {e}")

class DisneyRAGAssistant:
    """è¿ªå£«å°¼RAGæ£€ç´¢åŠ©æ‰‹ç±» - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self, index_dir: str = 'final_index', dashscope_api_key: str = None):
        """åˆå§‹åŒ–RAGåŠ©æ‰‹"""
        self.index_dir = index_dir
        self.inverted_index = {}
        self.chunk_mapping = []   
        self.vector_index = None  
        self.embeddings = None    
        self.embedding_model = None  
        self.rerank_model = None  
        self.ai_client = None  
        self.dashscope_api_key = dashscope_api_key  
        self.initialized = False
        
        # ä¼šè¯å†å²ç®¡ç†
        self.conversation_history = []
        self.max_history_length = 5
        
        # åœç”¨è¯è¡¨ - ç®€åŒ–ç‰ˆ
        self.stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
            'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š',
            'a', 'an', 'the', 'is', 'are', 'and', 'or', 'but', 'for', 'with',
            'å—', 'å‘¢', 'å§', 'å•Š', 'è¯·é—®', 'èƒ½å¦', 'æ˜¯å¦', 'æ€ä¹ˆ', 'å¦‚ä½•'
        }
        
        # æ¨¡å‹æ›´æ–°ç›¸å…³é…ç½®
        self.update_config_path = os.path.join(cache_dir, 'model_update_config.json')
        self.update_interval_days = 30  # æ¯æœˆæ£€æŸ¥ä¸€æ¬¡æ›´æ–°
        
        # æ··åˆæ£€ç´¢ç›¸å…³
        self.hybrid_retriever = None
        self.hybrid_retrieval_enabled = False
    
    def check_for_model_updates(self):
        """æ£€æŸ¥æ¨¡å‹æ›´æ–°
        
        æ¯æœˆè‡ªåŠ¨æ£€æŸ¥ä¸€æ¬¡æ¨¡å‹æ›´æ–°ï¼Œé€šè¿‡è®°å½•ä¸Šæ¬¡æ›´æ–°æ—¶é—´æ¥æ§åˆ¶æ£€æŸ¥é¢‘ç‡
        """
        import datetime
        import json
        
        # è¯»å–æˆ–åˆå§‹åŒ–æ›´æ–°é…ç½®
        last_update_time = None
        current_time = datetime.datetime.now()
        
        if os.path.exists(self.update_config_path):
            try:
                with open(self.update_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    if 'last_update_time' in config:
                        last_update_time = datetime.datetime.fromisoformat(config['last_update_time'])
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–è¶…è¿‡æ›´æ–°é—´éš”ï¼‰
        should_update = False
        if last_update_time is None:
            should_update = True
            print("ğŸ”„ é¦–æ¬¡è¿è¡Œï¼Œå°†æ£€æŸ¥æ¨¡å‹æ›´æ–°")
        else:
            days_since_update = (current_time - last_update_time).days
            if days_since_update >= self.update_interval_days:
                should_update = True
                print(f"ğŸ”„ è·ç¦»ä¸Šæ¬¡æ›´æ–°å·²è¶…è¿‡ {days_since_update} å¤©ï¼Œå°†æ£€æŸ¥æ¨¡å‹æ›´æ–°")
            else:
                print(f"âœ… æ¨¡å‹æ›´æ–°æ£€æŸ¥è·³è¿‡ï¼Œè·ç¦»ä¸‹æ¬¡æ›´æ–°è¿˜æœ‰ {self.update_interval_days - days_since_update} å¤©")
        
        if should_update:
            try:
                print("ğŸ”„ æ­£åœ¨æ£€æŸ¥å¹¶æ›´æ–°æœ¬åœ°æ¨¡å‹...")
                from huggingface_hub import snapshot_download
                
                # æ¨¡å‹åç§°æ˜ å°„
                models_to_update = [
                    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
                ]
                
                for model_name in models_to_update:
                    # ä¸‹è½½æœ€æ–°æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•
                    model_path = os.path.join(cache_dir, model_name.replace('/', '_'))
                    if not os.path.exists(model_path):
                        print(f"  ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹: {model_name}")
                        try:
                            snapshot_download(
                                repo_id=model_name,
                                cache_dir=cache_dir,
                                local_dir=model_path
                            )
                            print(f"  âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_name}")
                        except Exception as e:
                            print(f"  âš ï¸ æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_name}, é”™è¯¯: {str(e)}")
                    else:
                        print(f"  âœ… æ¨¡å‹å·²å­˜åœ¨: {model_name}")
                
                # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
                with open(self.update_config_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'last_update_time': current_time.isoformat(),
                        'update_interval_days': self.update_interval_days
                    }, f, ensure_ascii=False, indent=2)
                
                print("âœ… æ¨¡å‹æ›´æ–°æ£€æŸ¥å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                print("   ç¨‹åºå°†ç»§ç»­ä½¿ç”¨å·²æœ‰çš„æœ¬åœ°æ¨¡å‹")
    
    def load_index(self):
        """åŠ è½½æœç´¢ç´¢å¼• - ç®€åŒ–ç‰ˆ"""
        print(f"æ­£åœ¨åŠ è½½è¿ªå£«å°¼RAGçŸ¥è¯†åº“ç´¢å¼•...")
        
        # 1. å®‰è£…å¿…è¦çš„ä¾èµ–
        self._install_required_dependencies()
        
        # 2. æ£€æŸ¥æ¨¡å‹æ›´æ–°ï¼ˆæ¯æœˆä¸€æ¬¡ï¼‰
        self.check_for_model_updates()
        
        # 2. åŠ è½½åŸºæœ¬æ•°æ®
        try:
            # åŠ è½½å€’æ’ç´¢å¼•å’Œåˆ‡ç‰‡æ˜ å°„
            index_path = os.path.join(self.index_dir, 'simple_index.json')
            with open(index_path, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            self.inverted_index = index_data['inverted_index']
            
            # åŠ è½½åˆ‡ç‰‡æ˜ å°„
            mapping_path = os.path.join(self.index_dir, 'chunk_mapping.json')
            with open(mapping_path, 'r', encoding='utf-8') as f:
                self.chunk_mapping = json.load(f)
            
            print(f"âœ… åŸºæœ¬æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"- ç´¢å¼•è¯æ•°é‡: {len(self.inverted_index)}")
            print(f"- çŸ¥è¯†åº“åˆ‡ç‰‡æ•°: {len(self.chunk_mapping)}")
            
        except Exception as e:
            print(f"âŒ åŸºæœ¬æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            self.initialized = False
            return
        
        # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’Œå‘é‡ç´¢å¼•
        vector_search_enabled = False
        try:
            # å°è¯•åŠ è½½åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
            print("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            
            # ä½¿ç”¨å®ä¾‹çº§ç¼“å­˜ç›®å½•ï¼Œæˆ–åˆ›å»ºé»˜è®¤ç¼“å­˜ç›®å½•
            instance_cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.cache', 'huggingface')
            os.makedirs(instance_cache_dir, exist_ok=True)
            
            # å°è¯•ä½¿ç”¨sentence-transformersåº“åŠ è½½æ¨¡å‹
            try:
                # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
                local_model_path = os.path.join(instance_cache_dir, 'sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2')
                print(f"  ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {local_model_path}")
                
                # å°è¯•ä½¿ç”¨sentence-transformersç›´æ¥åŠ è½½æœ¬åœ°æ¨¡å‹
                self.embedding_model = SentenceTransformer(
                    local_model_path,
                    cache_folder=instance_cache_dir
                )
                print("  âœ… æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                print("  æ³¨æ„ï¼šç”±äºç½‘ç»œè¿æ¥é—®é¢˜æˆ–åº“ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œæ— æ³•åŠ è½½å‘é‡åµŒå…¥æ¨¡å‹")
                print("  ç³»ç»Ÿå°†å›é€€åˆ°å…³é”®è¯æœç´¢æ¨¡å¼ï¼Œè¿™å¯èƒ½å½±å“æœç´¢è´¨é‡ä½†ä»å¯ä½¿ç”¨åŸºæœ¬åŠŸèƒ½")
                # è®¾ç½®embedding_modelä¸ºNoneä»¥ä¾¿ç³»ç»Ÿèƒ½æ­£ç¡®æ£€æµ‹åˆ°æ¨¡å‹åŠ è½½å¤±è´¥å¹¶å›é€€åˆ°å…³é”®è¯æœç´¢
                self.embedding_model = None
                  
            # æ ¹æ®å®é™…åŠ è½½çŠ¶æ€æ‰“å°æ­£ç¡®çš„ä¿¡æ¯
            if self.embedding_model is not None:
                print(f"âœ… æˆåŠŸåŠ è½½åµŒå…¥æ¨¡å‹")
            else:
                print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")
            
            # å°è¯•åŠ è½½æˆ–åˆ›å»ºå‘é‡ç´¢å¼•
            vector_index_path = os.path.join(self.index_dir, 'vector_index.faiss')
            embeddings_path = os.path.join(self.index_dir, 'embeddings.npy')
            
            if os.path.exists(vector_index_path) and os.path.exists(embeddings_path):
                try:
                    self.vector_index = faiss.read_index(vector_index_path)
                    self.embeddings = np.load(embeddings_path)
                    print("âœ… æˆåŠŸåŠ è½½é¢„æ„å»ºçš„å‘é‡ç´¢å¼•å’ŒåµŒå…¥å‘é‡")
                except Exception as load_e:
                    print(f"âš ï¸ åŠ è½½é¢„æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(load_e)}ï¼Œå°†é‡æ–°åˆ›å»º")
                    self._create_vector_index()
            else:
                print("æ­£åœ¨åˆ›å»ºFAISSå‘é‡ç´¢å¼•...")
                self._create_vector_index()
            
            vector_search_enabled = True
            print("âœ… å‘é‡æœç´¢åŠŸèƒ½å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ å‘é‡æœç´¢åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print("   å°†ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")
            self.vector_index = None
            self.embedding_model = None
        
        # 3. åˆå§‹åŒ–ç®€å•çš„TF-IDFé‡æ’åº
        try:
            print("æ­£åœ¨åˆå§‹åŒ–TF-IDFé‡æ’åº...")
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹
            documents = [chunk['content'] for chunk in self.chunk_mapping]
            
            # åˆå§‹åŒ–TF-IDF
            self.tfidf_vectorizer = TfidfVectorizer(
                token_pattern=r'[\u4e00-\u9fa5]+|[a-zA-Z]+|[0-9]+',
                max_features=5000
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
            
            print("âœ… TF-IDFé‡æ’åºåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ TF-IDFé‡æ’åºåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # 4. åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆä¼˜å…ˆä½¿ç”¨OpenAIï¼Œå…¶æ¬¡æ˜¯é˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰
        print("ğŸ”„ å¼€å§‹åˆå§‹åŒ–AIå®¢æˆ·ç«¯...")
        self.ai_client = None
        
        try:
            # å¯¼å…¥httpxåº“ï¼Œç”¨äºåˆ›å»ºè‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯
            import httpx
            
            # åŠ¨æ€å¯¼å…¥OpenAIï¼Œç¡®ä¿ç‰ˆæœ¬å…¼å®¹
            try:
                from openai import OpenAI
                print("  âœ… OpenAIæ¨¡å—å¯¼å…¥æˆåŠŸ")
            except ImportError as e:
                print(f"  âš ï¸ OpenAIæ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
                print("  å°è¯•å®‰è£…æˆ–æ›´æ–°openaiåº“...")
                import subprocess
                import sys
                subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "openai==1.12.0"])
                from openai import OpenAI
                print("  âœ… é‡æ–°å¯¼å…¥OpenAIæˆåŠŸ")
            
            # ç›´æ¥ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
            print("  ğŸ” ç›´æ¥åˆå§‹åŒ–é˜¿é‡Œäº‘ç™¾ç‚¼APIå®¢æˆ·ç«¯")
            
            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
            self.dashscope_api_key = os.environ.get('ALIYUN_BAILIAN_API_KEY') or os.environ.get('DASHSCOPE_API_KEY')
            print(f"  ğŸ” æ£€æµ‹åˆ°é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥: {'å·²è®¾ç½®' if self.dashscope_api_key and not (self.dashscope_api_key.startswith('test_') or self.dashscope_api_key.startswith('your_')) else 'æœªè®¾ç½®æˆ–ä¸ºæµ‹è¯•å€¼'}")
            
            if self.dashscope_api_key:
                try:
                    print(f"  ğŸš€ å°è¯•ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥: {self.dashscope_api_key[:8]}...")
                    # åˆ›å»ºè‡ªå®šä¹‰httpxå®¢æˆ·ç«¯
                    custom_http_client = httpx.Client(timeout=30.0)
                    # ä½¿ç”¨è‡ªå®šä¹‰http_clientå’Œé˜¿é‡Œäº‘å¯†é’¥åˆå§‹åŒ–
                    # æ³¨æ„ï¼šhttpx.Clientä¸æ”¯æŒproxieså‚æ•°ï¼Œä½¿ç”¨æ ‡å‡†å‚æ•°
                    self.ai_client = OpenAI(
                        api_key=self.dashscope_api_key,
                        http_client=custom_http_client,
                        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # ç›´æ¥åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®base_url
                    )
                    # base_urlå·²åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
                    print("  âœ… å·²åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼APIåŸºç¡€URL")
                    print("âœ… é˜¿é‡Œäº‘ç™¾ç‚¼APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ é˜¿é‡Œäº‘ç™¾ç‚¼APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                    self.ai_client = None
            else:
                print("âš ï¸ é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥æ ¼å¼æ— æ•ˆæˆ–ä¸ºæµ‹è¯•å€¼")
                self.ai_client = None
        except Exception as e:
            print(f"âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¼‚å¸¸: {str(e)}")
            self.ai_client = None
        
        if not self.ai_client:
            print("ğŸ’¡ æœªè®¾ç½®æœ‰æ•ˆçš„APIå¯†é’¥ï¼ŒRAGç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨")
            print("   è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®æ­£ç¡®çš„ALIYUN_BAILIAN_API_KEY")
        
        # è®¾ç½®åˆå§‹åŒ–çŠ¶æ€
        self.initialized = True
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"- æœç´¢æ¨¡å¼: {'å‘é‡æœç´¢+å…³é”®è¯æœç´¢' if vector_search_enabled else 'ä»…å…³é”®è¯æœç´¢'}")
        print(f"- RAGç”ŸæˆåŠŸèƒ½: {'å·²å¯ç”¨' if self.ai_client else 'å·²ç¦ç”¨'}")
    
    def _create_vector_index(self):
        """åˆ›å»ºFAISSå‘é‡ç´¢å¼•"""
        # ç”ŸæˆåµŒå…¥å‘é‡
        chunk_texts = [chunk['content'] for chunk in self.chunk_mapping]
        self.embeddings = self.embedding_model.encode(chunk_texts, show_progress_bar=True)
        
        # åˆ›å»ºå¹¶æ·»åŠ åˆ°FAISSç´¢å¼•
        dimension = self.embeddings.shape[1]
        self.vector_index = faiss.IndexFlatL2(dimension)
        self.vector_index.add(self.embeddings.astype(np.float32))
        
        print(f"âœ… FAISSå‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {self.embeddings.shape[0]} ä¸ªå‘é‡")
    
    def _install_required_dependencies(self):
        """å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…"""
        print("ğŸ”„ æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–...")
        try:
            import subprocess
            import sys
            
            # å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
            required_packages = [
                'transformers>=4.30.0',
                'sentence-transformers>=2.2.0',
                'huggingface_hub>=0.14.0',
                'python-dotenv>=1.0.0'
            ]
            
            for package in required_packages:
                try:
                    print(f"  æ£€æŸ¥ {package}...")
                    # å°è¯•å¯¼å…¥åŒ…ï¼Œå¦‚æœæˆåŠŸåˆ™è·³è¿‡å®‰è£…
                    if package.split('>=')[0] in ['transformers', 'sentence-transformers', 'huggingface_hub']:
                        __import__(package.split('>=')[0])
                    elif package.startswith('python-dotenv'):
                        __import__('dotenv')
                    print(f"  âœ… {package} å·²å®‰è£…")
                except ImportError:
                    print(f"  ğŸ“¦ æ­£åœ¨å®‰è£… {package}...")
                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
                    print(f"  âœ… {package} å®‰è£…å®Œæˆ")
            
            print("âœ… ä¾èµ–æ£€æŸ¥å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ ä¾èµ–å®‰è£…è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("   ç¨‹åºå°†ç»§ç»­æ‰§è¡Œï¼Œä½†æŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    
    def preprocess_query(self, query: str) -> List[str]:
        """é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬ï¼Œæå–æœ‰æ•ˆå…³é”®è¯"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å¹¶æ ‡å‡†åŒ–
        query = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', query)
        query = query.strip().lower()
        
        # æå–æ‰€æœ‰å¯èƒ½çš„å…³é”®è¯
        keywords = []
        
        # æå–ä¸­æ–‡è¯ç»„ (2-4å­—)
        chinese_chars = re.findall(r'[\u4e00-\u9fa5]', query)
        for i in range(len(chinese_chars)):
            # ç”Ÿæˆ2-4å­—è¯ç»„
            for length in range(2, min(5, len(chinese_chars) - i + 1)):
                word = ''.join(chinese_chars[i:i+length])
                if word not in self.stop_words:
                    keywords.append(word)
        
        # æå–è‹±æ–‡å•è¯å’Œæ•°å­—
        en_num_pattern = r'[a-zA-Z]+|[0-9]+'
        en_num_words = re.findall(en_num_pattern, query)
        keywords.extend([w for w in en_num_words if w not in self.stop_words and len(w) > 1])
        
        # æ·»åŠ åŸå§‹æŸ¥è¯¢è¯ä½œä¸ºå…³é”®è¯
        if query and query not in self.stop_words:
            keywords.append(query)
        
        # å»é‡å¹¶è¿‡æ»¤æ‰å¤ªçŸ­çš„å…³é”®è¯
        keywords = [k for k in list(set(keywords)) if len(k) >= 2 or k.isdigit()]
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®è¯
        if not keywords:
            keywords = [query[:10]]  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢çš„å‰10ä¸ªå­—ç¬¦
        
        return keywords
    
    def vector_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ä½¿ç”¨FAISSå‘é‡åº“è¿›è¡Œè¯­ä¹‰æœç´¢"""
        if not self.initialized or not self.vector_index:
            print("âš ï¸ å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode([query])[0]
        
        # æ‰§è¡Œå‘é‡æœç´¢
        distances, indices = self.vector_index.search(np.array([query_vector]).astype(np.float32), top_k)
        
        results = []
        for i in range(len(indices[0])):
            idx = indices[0][i]
            distance = distances[0][i]
            
            # è·ç¦»è½¬ç›¸å…³æ€§åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸å…³ï¼‰
            score = 1.0 / (1.0 + distance)  # è½¬æ¢ä¸º0-1ä¹‹é—´çš„åˆ†æ•°
            
            chunk = self.chunk_mapping[idx]
            result = {
                'content': chunk['content'],
                'metadata': chunk['metadata'],
                'score': score,
                'distance': distance,
                'chunk_id': idx
            }
            results.append(result)
        
        print(f"å‘é‡æœç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        return results
    
    def rerank_results(self, query: str, candidates: List[Dict[str, Any]], top_k: int = 3) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œé‡æ’åº
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨é‡æ’åºæ¨¡å‹
        if self.rerank_model:
            try:
                print(f"ğŸ”„ ä½¿ç”¨å¤–éƒ¨é‡æ’åºæ¨¡å‹å¯¹ {len(candidates)} ä¸ªå€™é€‰æ–‡æ¡£è¿›è¡Œé‡æ’åº")
                start_time = time.time()
                
                # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼Œé™åˆ¶é•¿åº¦ä»¥é¿å…æ¨¡å‹é”™è¯¯
                pairs = []
                for doc in candidates:
                    # é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Œé¿å…æ¨¡å‹å¤„ç†è¿‡é•¿æ–‡æœ¬
                    content = doc['content'][:500]  # é™åˆ¶ä¸ºå‰500ä¸ªå­—ç¬¦
                    pairs.append((query[:100], content))  # é™åˆ¶æŸ¥è¯¢é•¿åº¦
                
                # ä½¿ç”¨é‡æ’åºæ¨¡å‹è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                scores = self.rerank_model.predict(pairs)
                
                # æ›´æ–°æ¯ä¸ªæ–‡æ¡£çš„åˆ†æ•°
                for i, doc in enumerate(candidates):
                    doc['rerank_score'] = float(scores[i])
                
                # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
                reranked_results = sorted(candidates, key=lambda x: x.get('rerank_score', 0), reverse=True)
                
                elapsed_time = time.time() - start_time
                print(f"âœ… å¤–éƒ¨é‡æ’åºå®Œæˆï¼Œä¿ç•™ {top_k} ä¸ªæœ€ç›¸å…³æ–‡æ¡£ (è€—æ—¶: {elapsed_time:.3f} ç§’)")
                
                # æ‰“å°é‡æ’åºåçš„åˆ†æ•°
                for i, doc in enumerate(reranked_results[:top_k]):
                    print(f"  {i+1}. é‡æ’åºåˆ†æ•°: {doc.get('rerank_score', 0):.4f}")
                
                return reranked_results[:top_k]
                
            except Exception as e:
                print(f"âŒ å¤–éƒ¨é‡æ’åºè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
                # æ— è®ºå¦‚ä½•éƒ½å°è¯•ä½¿ç”¨TF-IDFé‡æ’åºå¤‡é€‰æ–¹æ¡ˆ
                print("ğŸ”„ å°è¯•ä½¿ç”¨TF-IDFé‡æ’åºå¤‡é€‰æ–¹æ¡ˆ")
        
        # ä½¿ç”¨TF-IDFé‡æ’åºå¤‡é€‰æ–¹æ¡ˆï¼ˆä¸»è¦çš„é‡æ’åºé€»è¾‘ï¼‰
        if hasattr(self, 'tfidf_vectorizer') and hasattr(self, 'tfidf_matrix'):
            print("ğŸ”„ ä½¿ç”¨TF-IDFé‡æ’åºå¤‡é€‰æ–¹æ¡ˆ")
            return self._tfidf_rerank(query, candidates, top_k)
        
        # æœ€åçš„å¤‡é€‰ï¼šä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…é‡æ’åº
        print("âš ï¸ æ‰€æœ‰é‡æ’åºæ–¹æ¡ˆéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•å…³é”®è¯åŒ¹é…é‡æ’åº")
        try:
            # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†æ•°è®¡ç®—ï¼Œä¼˜åŒ–ä¸­æ–‡æ”¯æŒ
            # æå–ä¸­æ–‡å…³é”®è¯
            query_words = set(re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+|[0-9]+', query.lower()))
            
            for doc in candidates:
                # æå–æ–‡æ¡£ä¸­çš„è¯è¯­
                content_words = set(re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+|[0-9]+', doc['content'].lower()))
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…æ•°é‡
                common_words = query_words.intersection(content_words)
                # è®¡ç®—åŒ¹é…æ¯”ä¾‹
                keyword_match_score = len(common_words) / max(len(query_words), 1)
                
                # ç»“åˆåŸå§‹ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…åˆ†æ•°
                original_score = doc.get('score', 0)
                doc['rerank_score'] = 0.6 * original_score + 0.4 * keyword_match_score
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_results = sorted(candidates, key=lambda x: x.get('rerank_score', x.get('score', 0)), reverse=True)
            return reranked_results[:top_k]
        except Exception as e:
            print(f"âŒ ç®€å•é‡æ’åºè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›åŸå§‹å€™é€‰çš„å‰å‡ ä¸ª
            return candidates[:top_k]
    
    def _tfidf_rerank(self, query: str, candidates: List[Dict[str, Any]], top_k: int = 3) -> List[Dict[str, Any]]:
        """ä½¿ç”¨TF-IDFå¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œé‡æ’åº"""
        try:
            start_time = time.time()
            
            # è½¬æ¢æŸ¥è¯¢ä¸ºTF-IDFå‘é‡
            query_vector = self.tfidf_vectorizer.transform([query])
            
            # è®¡ç®—æ¯ä¸ªå€™é€‰æ–‡æ¡£çš„TF-IDFç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            
            # ä¸ºæ¯ä¸ªå€™é€‰æ–‡æ¡£è®¡ç®—TF-IDFç›¸ä¼¼åº¦
            for i, doc in enumerate(candidates):
                chunk_idx = doc.get('chunk_id')
                if chunk_idx is not None and chunk_idx < self.tfidf_matrix.shape[0]:
                    # ä½¿ç”¨chunk_idè·å–é¢„è®¡ç®—çš„æ–‡æ¡£å‘é‡
                    doc_vector = self.tfidf_matrix[chunk_idx]
                    tfidf_score = cosine_similarity(query_vector, doc_vector)[0][0]
                else:
                    # å¦‚æœæ²¡æœ‰chunk_idï¼Œç›´æ¥è®¡ç®—å½“å‰æ–‡æ¡£å†…å®¹çš„TF-IDF
                    doc_vector = self.tfidf_vectorizer.transform([doc['content']])
                    tfidf_score = cosine_similarity(query_vector, doc_vector)[0][0]
                
                # ç»“åˆåŸå§‹åˆ†æ•°å’ŒTF-IDFåˆ†æ•°
                original_score = doc.get('score', 0)
                doc['rerank_score'] = 0.6 * original_score + 0.4 * float(tfidf_score)
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åºå¹¶è¿”å›å‰top_kä¸ªç»“æœ
            reranked_results = sorted(candidates, key=lambda x: x.get('rerank_score', 0), reverse=True)
            return reranked_results[:top_k]
            
        except Exception:
            # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹åˆ†æ•°æ’åºä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            try:
                return sorted(candidates, key=lambda x: x.get('score', 0), reverse=True)[:top_k]
            except:
                # æœ€åå¤‡é€‰ï¼šè¿”å›åŸå§‹å€™é€‰çš„å‰å‡ ä¸ª
                return candidates[:top_k]
    
    def _basic_search(self, query: str, top_k: int = 5, min_score: float = 0.1, use_vector_search: bool = True, use_context: bool = True) -> List[Dict[str, Any]]:
        """åŸºç¡€æœç´¢æ–¹æ³• - ç”¨äºæ··åˆæ£€ç´¢å™¨çš„æœ¬åœ°æ£€ç´¢"""
        # é¢„å¤„ç†æŸ¥è¯¢
        rewritten_query = query
        if use_context and self.conversation_history and self.ai_client:
            rewritten_query = self.rewrite_query_with_context(query)
            if rewritten_query != query:
                print(f"ğŸ”„ æŸ¥è¯¢æ”¹å†™: '{query}' -> '{rewritten_query}'")
        
        # é¢„å¤„ç†æŸ¥è¯¢
        keywords = self.preprocess_query(rewritten_query)
        
        results = []
        recall_size = 10  # ç²—å¬å›æ•°é‡
        
        # 1. å‘é‡æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_vector_search and self.vector_index:
            vector_results = self.vector_search(rewritten_query, top_k=recall_size * 2)
            results.extend(vector_results)
        
        # 2. å…³é”®è¯æ£€ç´¢ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        chunk_scores = {}
        
        # åŸºäºç´¢å¼•çš„å…³é”®è¯åŒ¹é…
        for keyword in keywords:
            if keyword in self.inverted_index:
                for chunk_id in self.inverted_index[keyword]:
                    chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + 1.0
        
        # å…¨æ–‡æ¨¡ç³ŠåŒ¹é…
        for chunk_id, chunk in enumerate(self.chunk_mapping):
            content = chunk['content'].lower()
            metadata = chunk['metadata']
            filename = metadata.get('filename', '').lower()
            
            # è®¡ç®—å†…å®¹å’Œæ–‡ä»¶ååŒ¹é…å¾—åˆ†
            content_score = sum(content.count(keyword) * 0.5 for keyword in keywords if keyword in content)
            filename_score = sum(1.0 for keyword in keywords if keyword in filename)
            total_score = content_score + filename_score
            
            if total_score > 0:
                chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + total_score
        
        # æ·»åŠ å…³é”®è¯æ£€ç´¢ç»“æœ
        keyword_results = []
        for chunk_id, score in chunk_scores.items():
            final_score = min(1.0, score / len(keywords))
            if final_score >= min_score and chunk_id < len(self.chunk_mapping):
                chunk = self.chunk_mapping[chunk_id]
                keyword_results.append({
                    'content': chunk['content'],
                    'metadata': chunk['metadata'],
                    'score': final_score,
                    'is_keyword_match': True
                })
        
        # æŒ‰å¾—åˆ†æ’åºå¹¶æ·»åŠ åˆ°ç»“æœä¸­
        keyword_results.sort(key=lambda x: x['score'], reverse=True)
        results.extend(keyword_results[:recall_size])
        
        # 3. åˆå¹¶å¹¶å»é‡ç»“æœ
        unique_results = []
        seen_contents = set()
        
        # æŒ‰å¾—åˆ†æ’åºæ‰€æœ‰ç»“æœ
        results.sort(key=lambda x: x['score'], reverse=True)
        
        for result in results:
            # ä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼è¿›è¡Œå»é‡
            content_hash = hash(result['content'][:100])
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
                if len(unique_results) >= recall_size:
                    break
        
        # 4. é‡æ’åº
        reranked_results = self.rerank_results(rewritten_query, unique_results, top_k=top_k)
        
        return reranked_results
    
    def search(self, query: str, top_k: int = 5, min_score: float = 0.1, use_vector_search: bool = True, use_context: bool = True) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£ - æ”¯æŒæ··åˆæ£€ç´¢"""
        # å¦‚æœå¯ç”¨äº†æ··åˆæ£€ç´¢
        if self.hybrid_retrieval_enabled and self.hybrid_retriever:
            try:
                # æ‰§è¡Œæ··åˆæ£€ç´¢
                results, stats = self.hybrid_retriever.retrieve(
                    query,
                    top_k=top_k,
                    min_score=min_score,
                    use_vector_search=use_vector_search,
                    use_context=use_context
                )
                
                # è®°å½•æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
                if stats.get('used_web_search', False):
                    print(f"ğŸŒ æ··åˆæ£€ç´¢ - å·²ä½¿ç”¨ç½‘ç»œæœç´¢: {stats.get('trigger_reason', '')}")
                
                return results[:top_k]
            except Exception as e:
                print(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°æ£€ç´¢: {str(e)}")
        
        # å›é€€åˆ°åŸºæœ¬æœç´¢
        return self._basic_search(query, top_k, min_score, use_vector_search, use_context)
    
    def rewrite_query_with_context(self, query: str) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡å†å²æ”¹å†™æŸ¥è¯¢"""
        if not self.ai_client or not self.conversation_history:
            return query
        
        try:
            # æ„å»ºå†å²ä¸Šä¸‹æ–‡
            history_context = "\n".join([f"ç”¨æˆ·: {h['query']}\nåŠ©æ‰‹: {h['response_summary']}" for h in self.conversation_history[-3:]])
            
            # æ„å»ºæŸ¥è¯¢æ”¹å†™æç¤ºè¯
            prompt = f"""ä»»åŠ¡ï¼šå°†ä¸Šä¸‹æ–‡ä¾èµ–çš„ç”¨æˆ·æŸ¥è¯¢æ”¹å†™ä¸ºå®Œæ•´ç‹¬ç«‹çš„æŸ¥è¯¢ã€‚

ä¸Šä¸‹æ–‡å†å²:
{history_context}

å½“å‰ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·åŸºäºä¸Šä¸‹æ–‡å†å²ï¼Œå°†å½“å‰æŸ¥è¯¢æ”¹å†™ä¸ºä¸€ä¸ªå®Œæ•´ç‹¬ç«‹çš„æŸ¥è¯¢ï¼Œç¡®ä¿å®ƒåŒ…å«æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯ï¼Œä¸éœ€è¦ä¸Šä¸‹æ–‡å³å¯ç†è§£ã€‚æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ä¿ç•™åŸå§‹æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾ï¼Œä½†è¦æ‰©å±•å®ƒä»¥åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

è¾“å‡ºè¦æ±‚ï¼šä»…è¿”å›æ”¹å†™åçš„å®Œæ•´æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"""
            
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ”¹å†™ - å…¼å®¹ä¸åŒOpenAIç‰ˆæœ¬
            response = None
            try:
                # å°è¯•æ–°çš„APIæ ¼å¼ (OpenAI v1.x)
                if hasattr(self.ai_client, 'chat') and hasattr(self.ai_client.chat, 'completions'):
                    response = self.ai_client.chat.completions.create(
                        model="qwen-turbo",
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™åŠ©æ‰‹ï¼Œæ“…é•¿å°†ä¸Šä¸‹æ–‡ä¾èµ–çš„æŸ¥è¯¢è½¬æ¢ä¸ºç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=200,
                        temperature=0.3,  # è¾ƒä½çš„æ¸©åº¦ï¼Œä¿æŒä¸€è‡´æ€§
                        timeout=15
                    )
                elif hasattr(self.ai_client, 'chat_completions'):
                    response = self.ai_client.chat_completions.create(
                        model="qwen-turbo",
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™åŠ©æ‰‹ï¼Œæ“…é•¿å°†ä¸Šä¸‹æ–‡ä¾èµ–çš„æŸ¥è¯¢è½¬æ¢ä¸ºç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=200,
                        temperature=0.3,  # è¾ƒä½çš„æ¸©åº¦ï¼Œä¿æŒä¸€è‡´æ€§
                        timeout=15
                    )
                else:
                    # å°è¯•æ—§çš„APIæ ¼å¼ (OpenAI v0.x)
                    response = self.ai_client.ChatCompletion.create(
                        model="qwen-turbo",
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™åŠ©æ‰‹ï¼Œæ“…é•¿å°†ä¸Šä¸‹æ–‡ä¾èµ–çš„æŸ¥è¯¢è½¬æ¢ä¸ºç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=200,
                        temperature=0.3,
                        timeout=15
                    )
            except Exception as inner_e:
                print(f"âŒ APIè°ƒç”¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(inner_e)}")
                return query
            
            rewritten = response.choices[0].message.content.strip()
            return rewritten if rewritten else query
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ”¹å†™å¤±è´¥: {str(e)}")
            return query
    
    def update_conversation_history(self, query: str, response_content: str):
        """æ›´æ–°ä¼šè¯å†å²
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            response_content: åŠ©æ‰‹å“åº”
        """
        # ä¸ºå“åº”ç”Ÿæˆæ‘˜è¦ï¼Œç”¨äºå­˜å‚¨åœ¨å†å²è®°å½•ä¸­
        response_summary = response_content[:100] + "..." if len(response_content) > 100 else response_content
        
        self.conversation_history.append({
            'query': query,
            'response_summary': response_summary,
            'timestamp': time.time()
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨æœ€å¤§é•¿åº¦å†…
        if len(self.conversation_history) > self.max_history_length:
            self.conversation_history = self.conversation_history[-self.max_history_length:]
    
    def clear_conversation_history(self):
        """æ¸…é™¤ä¼šè¯å†å²"""
        self.conversation_history = []
        print("âœ… ä¼šè¯å†å²å·²æ¸…é™¤")
    
    def setup_hybrid_retrieval(self, config: Dict = None):
        """è®¾ç½®æ··åˆæ£€ç´¢åŠŸèƒ½
        
        Args:
            config: æ··åˆæ£€ç´¢é…ç½®å‚æ•°
        """
        if not HAS_HYBRID_RETRIEVAL:
            print("âŒ æ··åˆæ£€ç´¢æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è®¾ç½®")
            return False
        
        try:
            # å®šä¹‰æœ¬åœ°æ£€ç´¢å™¨åŒ…è£…å™¨
            class RAGRetrieverWrapper:
                def __init__(self, assistant):
                    self.assistant = assistant
                
                def retrieve(self, query: str, **kwargs):
                    return self.assistant._basic_search(query, **kwargs)
            
            # åˆ›å»ºæœ¬åœ°æ£€ç´¢å™¨åŒ…è£…å™¨
            rag_retriever_wrapper = RAGRetrieverWrapper(self)
            
            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
            self.hybrid_retriever = HybridRetriever(
                rag_retriever=rag_retriever_wrapper,
                config=config
            )
            
            self.hybrid_retrieval_enabled = True
            print("âœ… æ··åˆæ£€ç´¢åŠŸèƒ½å·²å¯ç”¨")
            return True
        except Exception as e:
            print(f"âŒ è®¾ç½®æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}")
            return False
    
    def toggle_hybrid_retrieval(self, enable: bool):
        """åˆ‡æ¢æ··åˆæ£€ç´¢åŠŸèƒ½çš„å¼€å…³
        
        Args:
            enable: æ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢
        """
        if not HAS_HYBRID_RETRIEVAL:
            print("âŒ æ··åˆæ£€ç´¢æ¨¡å—ä¸å¯ç”¨")
            return False
        
        # å¦‚æœè¦å¯ç”¨ä½†è¿˜æ²¡æœ‰åˆå§‹åŒ–
        if enable and not self.hybrid_retriever:
            return self.setup_hybrid_retrieval()
        
        self.hybrid_retrieval_enabled = enable
        print(f"âœ… æ··åˆæ£€ç´¢å·²{'å¯ç”¨' if enable else 'ç¦ç”¨'}")
        return True
    
    def generate_rag_response(self, query: str, top_k: int = 3) -> Dict[str, Any]:
        """ä½¿ç”¨å¤§æ¨¡å‹ç”ŸæˆRAGå“åº”
        
        å®ç°å®Œæ•´çš„RAGæµç¨‹ï¼š
        1. ç”¨æˆ·æé—®
        2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        3. åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
        """
        if not self.ai_client:
            return {
                'success': False,
                'error': 'å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æä¾›APIå¯†é’¥'
            }
        
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.search(query, top_k=top_k)
            
            if not search_results:
                # å¦‚æœæ··åˆæ£€ç´¢å¯ç”¨ä½†æ²¡æœ‰ç»“æœï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ç½‘ç»œæœç´¢
                if self.hybrid_retrieval_enabled and self.hybrid_retriever:
                    print("ğŸ”„ å°è¯•ç›´æ¥ä½¿ç”¨ç½‘ç»œæœç´¢è·å–ç»“æœ")
                    try:
                        # å¼ºåˆ¶ä½¿ç”¨ç½‘ç»œæœç´¢
                        _, stats = self.hybrid_retriever.retriever.evaluate_query_temporal_need(query)
                        if hasattr(self.hybrid_retriever, 'web_searcher'):
                            search_results = self.hybrid_retriever.web_searcher.search(query, num_results=top_k)
                            # æ ¼å¼åŒ–ç»“æœ
                            formatted_results = []
                            for i, result in enumerate(search_results):
                                formatted_results.append({
                                    'content': result.get('snippet', ''),
                                    'title': result.get('title', ''),
                                    'score': 0.7 - (i * 0.1),
                                    'source': 'web_search'
                                })
                            search_results = formatted_results
                    except Exception as e:
                        print(f"âŒ å¼ºåˆ¶ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            
            if not search_results:
                return {
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯'
                }
            
            # 2. æ„å»ºæç¤ºè¯
            # åŒºåˆ†æœ¬åœ°å’Œç½‘ç»œç»“æœ
            context_parts = []
            for i, result in enumerate(search_results):
                source = result.get('source', 'local')
                source_tag = "[ç½‘ç»œ]" if 'web' in source else "[æœ¬åœ°]"
                title = result.get('title', '')
                content = result['content'][:300] if len(result['content']) > 300 else result['content']
                
                context_line = f"æ–‡æ¡£ {i+1} {source_tag}: {title}\n{content}"
                context_parts.append(context_line)
            
            context = "\n\n".join(context_parts)
            
            prompt = f"""ä½ æ˜¯è¿ªå£«å°¼ä¹å›­çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å·²çŸ¥ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {query}

è¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·è¯´'æˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜'ã€‚å›ç­”è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆè¿ªå£«å°¼ä¹å›­çš„æœåŠ¡é£æ ¼ã€‚"""
            
            # 3. è°ƒç”¨APIç”Ÿæˆç­”æ¡ˆ - å…¼å®¹ä¸åŒOpenAIç‰ˆæœ¬
            model_name = "qwen-turbo"
            response = None
            try:
                # å°è¯•æ–°çš„APIæ ¼å¼ (OpenAI v1.x)
                if hasattr(self.ai_client, 'chat') and hasattr(self.ai_client.chat, 'completions'):
                    response = self.ai_client.chat.completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯è¿ªå£«å°¼ä¹å›­çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œä¸“ä¸šã€å‹å¥½ä¸”ä¹äºåŠ©äººã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=500,
                        temperature=0.7,
                        timeout=30
                    )
                elif hasattr(self.ai_client, 'chat_completions'):
                    response = self.ai_client.chat_completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯è¿ªå£«å°¼ä¹å›­çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œä¸“ä¸šã€å‹å¥½ä¸”ä¹äºåŠ©äººã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=500,
                        temperature=0.7,
                        timeout=30
                    )
                else:
                    # å°è¯•æ—§çš„APIæ ¼å¼ (OpenAI v0.x)
                    response = self.ai_client.ChatCompletion.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯è¿ªå£«å°¼ä¹å›­çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œä¸“ä¸šã€å‹å¥½ä¸”ä¹äºåŠ©äººã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=500,
                        temperature=0.7,
                        timeout=30
                    )
            except Exception as inner_e:
                raise inner_e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç”±å¤–éƒ¨try-exceptæ•è·
            
            # 4. å¤„ç†å“åº”
            answer = response.choices[0].message.content
            
            # æ›´æ–°ä¼šè¯å†å²
            self.update_conversation_history(query, answer)
            
            return {
                'success': True,
                'answer': answer,
                'sources': [
                    {
                        'filename': result['metadata'].get('filename', 'æœªçŸ¥æ–‡ä»¶'),
                        'content': result['content'][:200] + '...',
                        'score': result.get('rerank_score', result.get('score', 0))
                    }
                    for result in search_results
                ],
                'model_used': model_name
            }
            
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ["authentication", "invalid api key"]):
                return {'success': False, 'error': "APIå¯†é’¥è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚"}
            elif any(keyword in error_msg for keyword in ["timeout", "connection"]):
                return {'success': False, 'error': "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"}
            else:
                return {'success': False, 'error': f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"}

    
    def format_result(self, result: Dict[str, Any], index: int) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        content = result['content']
        # ç®€åŒ–å†…å®¹é•¿åº¦é™åˆ¶é€»è¾‘
        display_content = content[:200] + ("..." if len(content) > 200 else "")
        
        metadata = result['metadata']
        filename = metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
        category = metadata.get('category', 'æœªçŸ¥åˆ†ç±»')
        score = result['score']
        
        formatted = f"\nğŸ“„ ç»“æœ {index + 1} (ç›¸å…³æ€§: {score:.2f})"
        formatted += f"\n   æ–‡ä»¶: {filename}"
        formatted += f"\n   åˆ†ç±»: {category}"
        formatted += f"\n   å†…å®¹: {display_content}"
        
        return formatted
    
    def search_and_display(self, query: str, top_k: int = 5):
        """æœç´¢å¹¶æ˜¾ç¤ºç»“æœ"""
        results = self.search(query, top_k, use_context=True)
        
        if not results:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            return
        
        print(f"\nğŸ“‹ æœç´¢ç»“æœ:")
        print("-" * 60)
        
        # ç”Ÿæˆå“åº”æ‘˜è¦ç”¨äºæ›´æ–°ä¼šè¯å†å²
        response_summary = "æ‰¾åˆ°ç›¸å…³ä¿¡æ¯" if results else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        if results:
            first_result = results[0]['content'][:50] + "..." if len(results[0]['content']) > 50 else results[0]['content']
            response_summary = f"æ‰¾åˆ°{len(results)}æ¡ç»“æœï¼Œæœ€ç›¸å…³çš„æ˜¯: {first_result}"
        
        # æ›´æ–°ä¼šè¯å†å²
        self.update_conversation_history(query, response_summary)
        
        for i, result in enumerate(results):
            print(self.format_result(result, i))
            print("-" * 60)
    
    def display_knowledge_base_stats(self):
        """æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.initialized:
            print("è¯·å…ˆåŠ è½½ç´¢å¼•")
            return
        
        print("\nğŸ“Š è¿ªå£«å°¼RAGçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"æ€»åˆ‡ç‰‡æ•°: {len(self.chunk_mapping)}")
        print(f"ç´¢å¼•è¯æ•°é‡: {len(self.inverted_index)}")
        print(f"å‘é‡ç´¢å¼•çŠ¶æ€: {'å·²åˆå§‹åŒ–' if self.vector_index else 'æœªåˆå§‹åŒ–'}")
        
        # ç»Ÿè®¡åˆ†ç±»ä¿¡æ¯
        categories = {}
        for chunk in self.chunk_mapping:
            category = chunk['metadata'].get('category', 'æœªçŸ¥åˆ†ç±»')
            categories[category] = categories.get(category, 0) + 1
        
        print("\nğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
        for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {category}: {count} ä¸ªåˆ‡ç‰‡")

def interactive_mode(assistant):
    """äº¤äº’å¼æœç´¢æ¨¡å¼"""
    print("\n=== è¿ªå£«å°¼RAGçŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ ===")
    print("ğŸ’¡ è¾“å…¥é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼Œ'stats'æŸ¥çœ‹ç»Ÿè®¡ï¼Œ'clear'æ¸…ç©ºå†å²ï¼Œ'exit'é€€å‡º")
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = input("\næ‚¨çš„é—®é¢˜: ").strip()
            
            # æ£€æŸ¥ç‰¹æ®Šå‘½ä»¤
            if query.lower() in ['exit', 'quit', 'q', 'é€€å‡º']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
                
            elif query.lower() in ['clear', 'c']:
                assistant.clear_conversation_history()
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
                
            elif query.lower() in ['stats', 's']:
                assistant.display_knowledge_base_stats()
                continue
                
            elif query.lower() in ['help', 'h', '?']:
                print("\nğŸ“‹ å‘½ä»¤è¯´æ˜:")
                print("- ç›´æ¥è¾“å…¥é—®é¢˜: è‡ªåŠ¨è¿›è¡ŒRAGç”Ÿæˆå›ç­”")
                print("- 'stats': æŸ¥çœ‹çŸ¥è¯†åº“ç»Ÿè®¡")
                print("- 'clear': æ¸…ç©ºå¯¹è¯å†å²")
                print("- 'exit': é€€å‡ºç³»ç»Ÿ")
                continue
                
            elif not query:
                print("ğŸ’¡ è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹")
                continue
            
            # æ‰§è¡ŒRAGç”Ÿæˆå“åº”
            response = assistant.generate_rag_response(query)
            if response['success']:
                print(f"\nâœ… å›ç­”:")
                print(response['answer'])
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, source in enumerate(response['sources']):
                    print(f"  {i+1}. {source['filename']} (ç›¸å…³åº¦: {source['score']:.2f})")
            else:
                # å¦‚æœRAGå¤±è´¥ï¼Œå°è¯•æ™®é€šæœç´¢
                print(f"âŒ {response['error']}")
                print("ğŸ”„ å°è¯•æ™®é€šæœç´¢...")
                assistant.search_and_display(query)
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")

def main():
    """
    ä¸»å‡½æ•°ï¼ŒåŒæ—¶ä½œä¸ºå‘½ä»¤è¡Œå·¥å…·å…¥å£ç‚¹
    æ”¯æŒé€šè¿‡ 'disney-rag' å‘½ä»¤è°ƒç”¨
    """
    # è®¾ç½®å·¥ä½œç›®å½•å¹¶åŠ è½½ç¯å¢ƒå˜é‡
    current_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(current_dir)
    
    # å°è¯•åŠ è½½.envæ–‡ä»¶
    if os.path.exists('.env'):
        try:
            from dotenv import load_dotenv
            load_dotenv()
        except ImportError:
            pass  # é™é»˜å¤„ç†ç¼ºå¤±çš„åº“
    
    # è·å–APIå¯†é’¥
    api_key = os.environ.get('ALIYUN_BAILIAN_API_KEY') or os.environ.get('DASHSCOPE_API_KEY')
    
    # åˆå§‹åŒ–å¹¶å¯åŠ¨åŠ©æ‰‹
    # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•ç›®å½•ï¼Œæ ¹æ®ç›®å½•ç»“æ„é€‰æ‹©'final_index'
    index_dir = os.path.join(current_dir, 'final_index')
    assistant = DisneyRAGAssistant(index_dir=index_dir, dashscope_api_key=api_key)
    assistant.load_index()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\nğŸš€ è¿ªå£«å°¼RAGåŠ©æ‰‹å¯åŠ¨æˆåŠŸï¼")
    print("ğŸ’¡ æ‚¨å¯ä»¥ç›´æ¥è¾“å…¥é—®é¢˜ï¼Œæˆ–è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š")
    print("   - 'help': æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print("   - 'exit'/'quit'/'q': é€€å‡ºç¨‹åº")
    print("   - 'stats': æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
    print("   - 'clear': æ¸…ç©ºå¯¹è¯å†å²")
    print("\nğŸ” ç¤ºä¾‹é—®é¢˜ï¼š")
    print("   1. ä¸Šæµ·è¿ªå£«å°¼ä¹å›­é—¨ç¥¨å¤šå°‘é’±ï¼Ÿ")
    print("   2. è¿ªå£«å°¼ä¹å›­æœ‰å“ªäº›å¿…ç©é¡¹ç›®ï¼Ÿ")
    print("\n" + "="*60)
    
    interactive_mode(assistant)

if __name__ == "__main__":
    main()